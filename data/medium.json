{"status":"ok","feed":{"url":"https://medium.com/feed/@weiyuan.liu","title":"Stories by Weiyuan Liu on Medium","link":"https://medium.com/@weiyuan.liu?source=rss-669623997d13------2","author":"","description":"Stories by Weiyuan Liu on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*4yUpqz8BhHFgJ2PdEKN3ZQ.png"},"items":[{"title":"What the CSS\u200a\u2014\u200aImplementing Trigonometry in SCSS, and lessons learnt","pubDate":"2019-06-30 12:04:49","link":"https://medium.com/swlh/what-the-css-implementing-trigonometry-in-scss-and-lessons-learnt-15703e24e677?source=rss-669623997d13------2","guid":"https://medium.com/p/15703e24e677","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/1998/1*ZLC2EASVKtT5hxWwlNlXOA.png","description":"<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/swlh/what-the-css-implementing-trigonometry-in-scss-and-lessons-learnt-15703e24e677?source=rss-669623997d13------2\"><img src=\"https://cdn-images-1.medium.com/max/1998/1*ZLC2EASVKtT5hxWwlNlXOA.png\" width=\"1998\"></a></p>\n<p class=\"medium-feed-snippet\">CSS, at its core, is a style sheet language. But we can do so much more by using Sass with its SCSS syntax.</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/swlh/what-the-css-implementing-trigonometry-in-scss-and-lessons-learnt-15703e24e677?source=rss-669623997d13------2\">Continue reading on The Startup \u00bb</a></p>\n</div>","content":"<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/swlh/what-the-css-implementing-trigonometry-in-scss-and-lessons-learnt-15703e24e677?source=rss-669623997d13------2\"><img src=\"https://cdn-images-1.medium.com/max/1998/1*ZLC2EASVKtT5hxWwlNlXOA.png\" width=\"1998\"></a></p>\n<p class=\"medium-feed-snippet\">CSS, at its core, is a style sheet language. But we can do so much more by using Sass with its SCSS syntax.</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/swlh/what-the-css-implementing-trigonometry-in-scss-and-lessons-learnt-15703e24e677?source=rss-669623997d13------2\">Continue reading on The Startup \u00bb</a></p>\n</div>","enclosure":{},"categories":["front-end-development","css","design","coding","web-development"]},{"title":"Vikathon\u200a\u2014\u200aHacking run, having fun","pubDate":"2019-06-24 03:31:06","link":"https://blog.viki.com/vikathon-hacking-run-having-fun-3756339221d4?source=rss-669623997d13------2","guid":"https://medium.com/p/3756339221d4","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*hMNIfpjpY_vJSPsjOEYdqg.png","description":"\n<h3>Vikathon\u200a\u2014\u200aHacking run, having\u00a0fun</h3>\n<h4>Code.Code.Code.Code.Code.FUN.Code.Code.Code.Code.FUN</h4>\n<p>Why do we do hackathons? Frankly speaking, I believe that there is no one \u201cright\u201d answer. It can be for serious reasons, creating POCs that tackle global problems, to silly reasons\u200a\u2014\u200alike creating meme generators that aim to entertain the\u00a0masses.</p>\n<p>But it is not just about the results. The journey to attaining our goals in a short timespan, building camaraderie with teammates, and gaining new knowledge, are just as important.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hMNIfpjpY_vJSPsjOEYdqg.png\"><figcaption>Fun times at\u00a0Viki!</figcaption></figure><p>Viki had previously organised a <a href=\"https://engineering.viki.com/blog/2014/hackathon-at-viki/\">hackathon</a> in celebration of this culture. This time, Viki embraced this tradition for the newer generation of Vikians, stylizing the internal hackathon as \u201cVikathon\u201d. Any employee could participate in this two day event, up to a maximum of three for each\u00a0team.</p>\n<p>Hitting close to home, this Vikathon\u2019s theme was focused on improving or reinventing our current services. This was an opportunity for developers such as myself, to pitch for a direction or area of interest that the company should advance towards. At the end of the two days, teams will be judged on three different areas - impact, technical depth and creativity.</p>\n<p>I\u2019m Weiyuan and this is my story for this Vikathon\u00a0\ud83d\ude1d</p>\n<h3>Day 0: <strong>The Day before \u201cVikathon\u201d</strong>\n</h3>\n<p>There are hushed tones of excitement permeating throughout our workstations. Different teams were gathering in their individual spaces, discussing how to execute their visions over the next couple\u00a0days.</p>\n<p>For myself, it was another ordinary day. I did not sign up for the event simply because I did not had any interesting ideas to work\u00a0on.</p>\n<blockquote>\u201cChocolate abs? Is that a new kind of\u00a0candy?\u201d</blockquote>\n<blockquote>\u2014 Some guy. I cannot confirm or deny if I am that\u00a0person.</blockquote>\n<p>That afternoon, I was approached by Kristie, my friend and colleague + community experience specialist. Now, Kristie is a person of many \u201cinteresting\u201d ideas, but what she suggested now redefined that word entirely.</p>\n<blockquote>\u201cWeiyuan, I need you to build something that that can recognize my Oppa\u2019s chocolate abs!\u201d</blockquote>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/150/0*t4XfYET4DuaRoMEf.png\"></figure><blockquote>\u201cWait, what?!!\u201d</blockquote>\n<p>After further discussion, I finally understood Kristie\u2019s ideals. She wanted to examine different K-Dramas, and \u201ccollect\u201d scenes where the male body was <em>*ahem*</em> \u201cglorified\u201d. These scenes will be saved within the celebrity collections, for their fans to view at their\u00a0leisure.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZsVGTKb_b6B9t-5YG4rtVg.png\"><figcaption>Not everyday you get to see someone Febreze their abs. Only in K-Dramas\u00a0XD</figcaption></figure><p>Since having a full team was beneficial for getting more things done during the hackathon, we decided to scout <a href=\"https://sg.linkedin.com/in/uyjulius\">Julius</a> as our last member, who was a mobile engineer at\u00a0Viki.</p>\n<p>And so the team was formed\u2026\u00a0\ud83d\ude04</p>\n<p>Later that night, we explored deeper and realized the underlying potentials of Kristie\u2019s ideas. Abs recognized from a video via object recognition, lends credence to the untapped prospects of a VOD service, such as automatically generated highlights related to users\u2019 interests. It could also benefit our sister service, <a href=\"https://sports.rakuten.com/\">Rakuten Sports</a>, in automating highlight discovery for significant events, such as the act of scoring\u00a0goals.</p>\n<p>Another aspect was that facial recognition of celebrities could contribute to a new measurement relating to their involvement, complementing our current systems of rating scores and genre categories. For example, we could recommend fans of a certain celebrity other dramas, which will be sorted by the same celebrity\u2019s screen time or involvement.</p>\n<p>The problem was that none of us had any background in ML, or object + facial recognition. But hey, that\u2019s what the hackathon is\u00a0about!</p>\n<h3><strong>Vikathon\u200a\u2014\u200aDay 1, starting the hackathon</strong></h3>\n<p>The hackathon started formally after a company all-hands. Participants gathered at the event hall for this Vikathon, to work on their\u00a0ideas.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ux19p_6eDkZldIATgNANjQ.png\"></figure><p>For my team, we divided the work according to our strengths. Julius would work on a mobile prototype that will showcase the presence of a celebrity and the \u201cabs\u201d highlights within a video feed. I would work on understanding how to harness object and facial recognition on a video input, and then outputting the various highlights in JSON format to interface with Julius\u2019s planned implementation. Kristie would work on automation to gather images in training our models (and facial classifier)\u200a\u2014\u200aand as our leader, keeping us on the same page at all\u00a0times.</p>\n<p>Since I had little experience with object recognition, I started off with a series of <a href=\"https://www.youtube.com/watch?v=COlbP62-B-U\">video guides</a> (Note: I watched this till part 5), to gain better understanding on how I could utilize the open-sourced <a href=\"https://github.com/tensorflow/models\">Tensorflow libraries and examples</a> to perform object recognition.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dLScXfx56Xlt7Yxmlf_TYw.png\"><figcaption>One of the initial tutorials linked from the video guides\u200a\u2014\u200adetecting a dog based on a existing\u00a0model</figcaption></figure><p>The tutorials were encouraging, producing what seems to be highly accurate results as seen above. From the tutorial, which was written in python, I modified the logic to run iteratively on frames from a video feed instead of a single image. Then the following happened:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/250/1*QGNLaixDh_NVBgd5EIBfEA.gif\"><figcaption>Recognition code works on videos but needs training. Time to train our models to recognizes abs!</figcaption></figure><p>Since our intentions was to build a POC in recognizing \u201cabs\u201d, we could discard the inaccurate model used above, and refocus our efforts in training for an intended\u00a0model.</p>\n<p>While waiting for the images to be collected for training our \u201cabs\u201d model, I started work on understanding facial recognition. Object recognition and facial recognition stem from the same ML concepts. However, we could not utilize the same code from above, as facial recognition differ from object recognition in some ways. One way to describe the difference is that a face can be an object, but the distinctive features of a face (distance between features, shape, color and expressions) decides the actual identity of the same\u00a0face.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*BOFw2jFWubvkaSO1Q0r7Cw.png\"><figcaption>Noona! Oh yes facial recognition works!</figcaption></figure><p>Fortunately, there were <a href=\"https://github.com/ageitgey/face_recognition\">facial recognition libraries</a> out there that already solved the facial recognition problem. Following the same process with learning how to perform facial recognition as linked from the repository above, I experimented and endeavored to <a href=\"https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_knn.py\">train our own classifier</a> from the base model to recognize our fans\u2019 \u201cOppas\u201d and \u201cNoonas\u201d\u00a0better.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9_2A0P2MNy_A_cLPv5MbMA.jpeg\"></figure><p>At this point, the sun was setting and time for the first day was running out. Our team caught up on each of our individual progress\u200a\u2014\u200aJulius showed us a prototype augmented from the current Viki\u2019s android application with clickable highlights of a target celebrity.</p>\n<p>Kristie also managed to automate the process for downloading our training images from Google. While the results were beautiful, for the next two days, my \u201cDownloads\u201d folder basically became the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8EDm1yPBYFsc60pa-Cem0g.png\"><figcaption>As a healthy married male to a beautiful wife, I hope my wife doesn\u2019t discover\u00a0this\u2026</figcaption></figure><p>With the gathered images, we started the training process which progressed throughout the\u00a0night.</p>\n<h3><strong>Vikathon\u200a\u2014\u200aDay 2, finishing up and presentations</strong></h3>\n<p>Day 2 started with some mishaps\u200a\u2014\u200athe abs model was not sufficiently trained to provide for a high enough accuracy in recognizing abs. I decided to invest more time to train the model for more iterations. Things turned around and we managed to get more accurate content by\u00a0noon.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Om-59Zq63fCpwS7NIgMlFw.png\"><figcaption>Chocolate abs v1, improvement in detection to\u00a0v2</figcaption></figure><p>(On the side) We started messing with the trained model, which produced some amusing\u00a0results:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/464/1*H9QTIj6svl2Jv-jsl3_f8Q.png\"></figure><p>While waiting for the abs recognition model to finish training on my machine, I worked on getting the JSON output from the trained facial recognition classifier. Based on the facial recognition results, I added tolerance weights to each frame. This ensured that celebrities only needed to appear for a proportion of the frames in the assigned second, in order for the presence criteria to kick in. Further computation is performed to transformed the presence of each second to time ranges (representing a highlight clip).</p>\n<p>To boost the accuracy of the gathered highlights further, I added in stitching logic to ensure that highlights that were considered close (below a defined time threshold) are combined. This helped to reduce the fragmentation of highlights caused by inaccurate data points and short transitions within the\u00a0video.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fUZim1aYNzt9MXrPcEIzqQ.png\"><figcaption>Each classified celebrity returns a list of time ranges, representing the highlights</figcaption></figure><p>To top the icing on the cake, scoring criteria was also added as discussed previously. This was measured as the total time presence, as well as close and far shots, based on the size of the face recognized in the\u00a0video.</p>\n<p>Combining the above logic with the trained abs recognition, we managed to achieve a POC (abs to be worked on!) of our intended objectives:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Js6i1S3DLm6Y4q8q\"></figure><p>Wrapping up our development, Julius also shared his updates on a fully fledged application with the playable highlights. This application was also equipped with a home page POC with suggested content that fans could be interested in perusing, which we planned to show during the judging processions.</p>\n<h3><strong>Presentations</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zmhzAq0aCIRcDRGIJQkjCg.png\"></figure><p>For the judging process, two judges were selected from within Viki, and another two judges from Rakuten. Each team was given some time to share their end product with the judges and the audience, and the motivation behind realizing their visions through this Vikathon.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EWAIbX9p9Y_88sMbx5YMdw.png\"></figure><p>A total of 14 teams submitted their ideas to be presented. The ideas ranged from fun ideas, such as allowing users to interact with the video feed (throwing hearts when a certain character shows up), to ideas relating our day to day pain points such as a Spinnaker themed Continuous Delivery and/or Deployment setup (dependent on environment).</p>\n<p>For my team, we settled on the theme as \u201cViki Moments\u201d, showcasing Julius POC mobile application combined with the derived results from parsing videos worked on by Kristie and\u00a0me.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/848/1*Xoq8-5Si-i-8DaHXr-tXUg.png\"></figure><p>Some of the presentations are currently being uploaded (and will be linked here\u00a0soon!)</p>\n<p><strong>After the judging process, the following winners were announced:</strong></p>\n<p><strong>Most Impactful</strong>\u200a\u2014\u200aThirst Quenchers (Julius, Weiyuan, Kristie) (Us!!!)<br><strong>Best Technical Innovation</strong>\u200a\u2014\u200aDishdash Master, Chrome extension to remind us to order our daily lunch as well as to provide suggestions (Candra, Longfei)<br><strong>Most Creative</strong>\u200a\u2014\u200aOMO, a new interactive video experience (Stephanie, Laura, Annabel)<br><strong>Special Rakuten Prize</strong>\u200a\u2014\u200aRIT new translations and real time\u00a0(Liling)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KimbqHRtXPG8xU4uR6X_og.png\"><figcaption>Yes we won the most impactful prize! ^^ From left: Alex (one of the judges), Kristie, Julius and Weiyuan\u00a0(me!)</figcaption></figure><p>Congratulations to all who\u00a0won!</p>\n<p>As the festivities drew to a close, there was much to ponder. Looking back, there was a lot that was accomplished over the two days by each of the participating teams. Some of the ideas even started to come to fruition just slightly under a month after the event, by the initiative of the related engineers. There was also talk on discussing rolling out some of the projects as formal features of our services.</p>\n<p>For myself, while I would not say that I am now a ML expert, there was much that I learn and grew to appreciate in this field over this short time span. There was also much solidarity and friendship exchanged between my team, with mutual support between teams in supporting each other during the presentation.</p>\n<p>I think this chalks up as a success for this Vikathon! Looking forward to the next iteration and new ideas in the coming\u00a0year!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3756339221d4\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/vikathon-hacking-run-having-fun-3756339221d4\">Vikathon\u200a\u2014\u200aHacking run, having fun</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>Vikathon\u200a\u2014\u200aHacking run, having\u00a0fun</h3>\n<h4>Code.Code.Code.Code.Code.FUN.Code.Code.Code.Code.FUN</h4>\n<p>Why do we do hackathons? Frankly speaking, I believe that there is no one \u201cright\u201d answer. It can be for serious reasons, creating POCs that tackle global problems, to silly reasons\u200a\u2014\u200alike creating meme generators that aim to entertain the\u00a0masses.</p>\n<p>But it is not just about the results. The journey to attaining our goals in a short timespan, building camaraderie with teammates, and gaining new knowledge, are just as important.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hMNIfpjpY_vJSPsjOEYdqg.png\"><figcaption>Fun times at\u00a0Viki!</figcaption></figure><p>Viki had previously organised a <a href=\"https://engineering.viki.com/blog/2014/hackathon-at-viki/\">hackathon</a> in celebration of this culture. This time, Viki embraced this tradition for the newer generation of Vikians, stylizing the internal hackathon as \u201cVikathon\u201d. Any employee could participate in this two day event, up to a maximum of three for each\u00a0team.</p>\n<p>Hitting close to home, this Vikathon\u2019s theme was focused on improving or reinventing our current services. This was an opportunity for developers such as myself, to pitch for a direction or area of interest that the company should advance towards. At the end of the two days, teams will be judged on three different areas - impact, technical depth and creativity.</p>\n<p>I\u2019m Weiyuan and this is my story for this Vikathon\u00a0\ud83d\ude1d</p>\n<h3>Day 0: <strong>The Day before \u201cVikathon\u201d</strong>\n</h3>\n<p>There are hushed tones of excitement permeating throughout our workstations. Different teams were gathering in their individual spaces, discussing how to execute their visions over the next couple\u00a0days.</p>\n<p>For myself, it was another ordinary day. I did not sign up for the event simply because I did not had any interesting ideas to work\u00a0on.</p>\n<blockquote>\u201cChocolate abs? Is that a new kind of\u00a0candy?\u201d</blockquote>\n<blockquote>\u2014 Some guy. I cannot confirm or deny if I am that\u00a0person.</blockquote>\n<p>That afternoon, I was approached by Kristie, my friend and colleague + community experience specialist. Now, Kristie is a person of many \u201cinteresting\u201d ideas, but what she suggested now redefined that word entirely.</p>\n<blockquote>\u201cWeiyuan, I need you to build something that that can recognize my Oppa\u2019s chocolate abs!\u201d</blockquote>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/150/0*t4XfYET4DuaRoMEf.png\"></figure><blockquote>\u201cWait, what?!!\u201d</blockquote>\n<p>After further discussion, I finally understood Kristie\u2019s ideals. She wanted to examine different K-Dramas, and \u201ccollect\u201d scenes where the male body was <em>*ahem*</em> \u201cglorified\u201d. These scenes will be saved within the celebrity collections, for their fans to view at their\u00a0leisure.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZsVGTKb_b6B9t-5YG4rtVg.png\"><figcaption>Not everyday you get to see someone Febreze their abs. Only in K-Dramas\u00a0XD</figcaption></figure><p>Since having a full team was beneficial for getting more things done during the hackathon, we decided to scout <a href=\"https://sg.linkedin.com/in/uyjulius\">Julius</a> as our last member, who was a mobile engineer at\u00a0Viki.</p>\n<p>And so the team was formed\u2026\u00a0\ud83d\ude04</p>\n<p>Later that night, we explored deeper and realized the underlying potentials of Kristie\u2019s ideas. Abs recognized from a video via object recognition, lends credence to the untapped prospects of a VOD service, such as automatically generated highlights related to users\u2019 interests. It could also benefit our sister service, <a href=\"https://sports.rakuten.com/\">Rakuten Sports</a>, in automating highlight discovery for significant events, such as the act of scoring\u00a0goals.</p>\n<p>Another aspect was that facial recognition of celebrities could contribute to a new measurement relating to their involvement, complementing our current systems of rating scores and genre categories. For example, we could recommend fans of a certain celebrity other dramas, which will be sorted by the same celebrity\u2019s screen time or involvement.</p>\n<p>The problem was that none of us had any background in ML, or object + facial recognition. But hey, that\u2019s what the hackathon is\u00a0about!</p>\n<h3><strong>Vikathon\u200a\u2014\u200aDay 1, starting the hackathon</strong></h3>\n<p>The hackathon started formally after a company all-hands. Participants gathered at the event hall for this Vikathon, to work on their\u00a0ideas.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ux19p_6eDkZldIATgNANjQ.png\"></figure><p>For my team, we divided the work according to our strengths. Julius would work on a mobile prototype that will showcase the presence of a celebrity and the \u201cabs\u201d highlights within a video feed. I would work on understanding how to harness object and facial recognition on a video input, and then outputting the various highlights in JSON format to interface with Julius\u2019s planned implementation. Kristie would work on automation to gather images in training our models (and facial classifier)\u200a\u2014\u200aand as our leader, keeping us on the same page at all\u00a0times.</p>\n<p>Since I had little experience with object recognition, I started off with a series of <a href=\"https://www.youtube.com/watch?v=COlbP62-B-U\">video guides</a> (Note: I watched this till part 5), to gain better understanding on how I could utilize the open-sourced <a href=\"https://github.com/tensorflow/models\">Tensorflow libraries and examples</a> to perform object recognition.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dLScXfx56Xlt7Yxmlf_TYw.png\"><figcaption>One of the initial tutorials linked from the video guides\u200a\u2014\u200adetecting a dog based on a existing\u00a0model</figcaption></figure><p>The tutorials were encouraging, producing what seems to be highly accurate results as seen above. From the tutorial, which was written in python, I modified the logic to run iteratively on frames from a video feed instead of a single image. Then the following happened:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/250/1*QGNLaixDh_NVBgd5EIBfEA.gif\"><figcaption>Recognition code works on videos but needs training. Time to train our models to recognizes abs!</figcaption></figure><p>Since our intentions was to build a POC in recognizing \u201cabs\u201d, we could discard the inaccurate model used above, and refocus our efforts in training for an intended\u00a0model.</p>\n<p>While waiting for the images to be collected for training our \u201cabs\u201d model, I started work on understanding facial recognition. Object recognition and facial recognition stem from the same ML concepts. However, we could not utilize the same code from above, as facial recognition differ from object recognition in some ways. One way to describe the difference is that a face can be an object, but the distinctive features of a face (distance between features, shape, color and expressions) decides the actual identity of the same\u00a0face.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*BOFw2jFWubvkaSO1Q0r7Cw.png\"><figcaption>Noona! Oh yes facial recognition works!</figcaption></figure><p>Fortunately, there were <a href=\"https://github.com/ageitgey/face_recognition\">facial recognition libraries</a> out there that already solved the facial recognition problem. Following the same process with learning how to perform facial recognition as linked from the repository above, I experimented and endeavored to <a href=\"https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_knn.py\">train our own classifier</a> from the base model to recognize our fans\u2019 \u201cOppas\u201d and \u201cNoonas\u201d\u00a0better.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9_2A0P2MNy_A_cLPv5MbMA.jpeg\"></figure><p>At this point, the sun was setting and time for the first day was running out. Our team caught up on each of our individual progress\u200a\u2014\u200aJulius showed us a prototype augmented from the current Viki\u2019s android application with clickable highlights of a target celebrity.</p>\n<p>Kristie also managed to automate the process for downloading our training images from Google. While the results were beautiful, for the next two days, my \u201cDownloads\u201d folder basically became the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8EDm1yPBYFsc60pa-Cem0g.png\"><figcaption>As a healthy married male to a beautiful wife, I hope my wife doesn\u2019t discover\u00a0this\u2026</figcaption></figure><p>With the gathered images, we started the training process which progressed throughout the\u00a0night.</p>\n<h3><strong>Vikathon\u200a\u2014\u200aDay 2, finishing up and presentations</strong></h3>\n<p>Day 2 started with some mishaps\u200a\u2014\u200athe abs model was not sufficiently trained to provide for a high enough accuracy in recognizing abs. I decided to invest more time to train the model for more iterations. Things turned around and we managed to get more accurate content by\u00a0noon.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Om-59Zq63fCpwS7NIgMlFw.png\"><figcaption>Chocolate abs v1, improvement in detection to\u00a0v2</figcaption></figure><p>(On the side) We started messing with the trained model, which produced some amusing\u00a0results:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/464/1*H9QTIj6svl2Jv-jsl3_f8Q.png\"></figure><p>While waiting for the abs recognition model to finish training on my machine, I worked on getting the JSON output from the trained facial recognition classifier. Based on the facial recognition results, I added tolerance weights to each frame. This ensured that celebrities only needed to appear for a proportion of the frames in the assigned second, in order for the presence criteria to kick in. Further computation is performed to transformed the presence of each second to time ranges (representing a highlight clip).</p>\n<p>To boost the accuracy of the gathered highlights further, I added in stitching logic to ensure that highlights that were considered close (below a defined time threshold) are combined. This helped to reduce the fragmentation of highlights caused by inaccurate data points and short transitions within the\u00a0video.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fUZim1aYNzt9MXrPcEIzqQ.png\"><figcaption>Each classified celebrity returns a list of time ranges, representing the highlights</figcaption></figure><p>To top the icing on the cake, scoring criteria was also added as discussed previously. This was measured as the total time presence, as well as close and far shots, based on the size of the face recognized in the\u00a0video.</p>\n<p>Combining the above logic with the trained abs recognition, we managed to achieve a POC (abs to be worked on!) of our intended objectives:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Js6i1S3DLm6Y4q8q\"></figure><p>Wrapping up our development, Julius also shared his updates on a fully fledged application with the playable highlights. This application was also equipped with a home page POC with suggested content that fans could be interested in perusing, which we planned to show during the judging processions.</p>\n<h3><strong>Presentations</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zmhzAq0aCIRcDRGIJQkjCg.png\"></figure><p>For the judging process, two judges were selected from within Viki, and another two judges from Rakuten. Each team was given some time to share their end product with the judges and the audience, and the motivation behind realizing their visions through this Vikathon.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EWAIbX9p9Y_88sMbx5YMdw.png\"></figure><p>A total of 14 teams submitted their ideas to be presented. The ideas ranged from fun ideas, such as allowing users to interact with the video feed (throwing hearts when a certain character shows up), to ideas relating our day to day pain points such as a Spinnaker themed Continuous Delivery and/or Deployment setup (dependent on environment).</p>\n<p>For my team, we settled on the theme as \u201cViki Moments\u201d, showcasing Julius POC mobile application combined with the derived results from parsing videos worked on by Kristie and\u00a0me.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/848/1*Xoq8-5Si-i-8DaHXr-tXUg.png\"></figure><p>Some of the presentations are currently being uploaded (and will be linked here\u00a0soon!)</p>\n<p><strong>After the judging process, the following winners were announced:</strong></p>\n<p><strong>Most Impactful</strong>\u200a\u2014\u200aThirst Quenchers (Julius, Weiyuan, Kristie) (Us!!!)<br><strong>Best Technical Innovation</strong>\u200a\u2014\u200aDishdash Master, Chrome extension to remind us to order our daily lunch as well as to provide suggestions (Candra, Longfei)<br><strong>Most Creative</strong>\u200a\u2014\u200aOMO, a new interactive video experience (Stephanie, Laura, Annabel)<br><strong>Special Rakuten Prize</strong>\u200a\u2014\u200aRIT new translations and real time\u00a0(Liling)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KimbqHRtXPG8xU4uR6X_og.png\"><figcaption>Yes we won the most impactful prize! ^^ From left: Alex (one of the judges), Kristie, Julius and Weiyuan\u00a0(me!)</figcaption></figure><p>Congratulations to all who\u00a0won!</p>\n<p>As the festivities drew to a close, there was much to ponder. Looking back, there was a lot that was accomplished over the two days by each of the participating teams. Some of the ideas even started to come to fruition just slightly under a month after the event, by the initiative of the related engineers. There was also talk on discussing rolling out some of the projects as formal features of our services.</p>\n<p>For myself, while I would not say that I am now a ML expert, there was much that I learn and grew to appreciate in this field over this short time span. There was also much solidarity and friendship exchanged between my team, with mutual support between teams in supporting each other during the presentation.</p>\n<p>I think this chalks up as a success for this Vikathon! Looking forward to the next iteration and new ideas in the coming\u00a0year!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3756339221d4\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/vikathon-hacking-run-having-fun-3756339221d4\">Vikathon\u200a\u2014\u200aHacking run, having fun</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["engineering","technology","hackathons","machine-learning","innovation"]},{"title":"Content Management tools, implemented with private browser extension","pubDate":"2019-06-16 06:44:36","link":"https://medium.com/@weiyuan.liu/an-alternative-to-support-internal-web-features-8a9a88fa452d?source=rss-669623997d13------2","guid":"https://medium.com/p/8a9a88fa452d","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*Ugcb2Izz4bQkfgOc7Cksfw.png","description":"\n<h3>Content\u00a0Management\u00a0tools, implemented with private browser extensions</h3>\n<h4>Making internal content management tools easier to\u00a0implement, and breaking lesser things in the\u00a0process.</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Ugcb2Izz4bQkfgOc7Cksfw.png\"><figcaption>Modified image, source:\u00a0<a href=\"https://svgsilh.com/image/34443.html\">svgsilh</a></figcaption></figure><p>If you have built some web service before, or in the process of planning to build one, you might have thought about putting in internal features on selected pages. Perhaps an <em>\u201cEdit Post\u201d</em> right-click menu option if you\u2019re building a news site, or <em>\u201cList Sellers\u201d</em> button if you\u2019re building an e-commerce site, both which could be linked to an another web\u00a0service.</p>\n<p>Developing those internal features is certainly\u00a0doable. But for features that cater to internal\u00a0users,\u00a0we\u00a0may\u00a0be\u00a0looking\u00a0at\u00a01%\u00a0or an\u00a0even\u00a0lesser proportion\u00a0of\u00a0the\u00a0audience. Implementation becomes costly in intangible ways, such as impacting regular users if bugs arise from these internal features.</p>\n<p>Here\u2019s my take on why it\u2019s better to implement these internal tools with private browser extensions.</p>\n<h3>1. We can incrementally add internal features, without impacting main user\u00a0groups</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*l7owKKVneKskw3RrmzxvQw.png\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/illustrations/under-construction-construction-sign-2408062/\">pixabay</a></figcaption></figure><p>Let\u2019s say we add a new process to our product site that is only meant for our staff. It raises some questions:</p>\n<blockquote>\u201cWhat if parts of the process are accidentally leaked to our clients? Or what if this process broke the page and affects all our\u00a0users?\u201d</blockquote>\n<p>In the\u00a0above\u00a0questions, we highlight the risk of creating bugs that are part and parcel of feature releases. For the observed\u00a0bugs, we have to spend resources to fix the tools, and rollback if necessary.</p>\n<p>However, the surface area for bugs expands as we add more internal tools to our web service,\u00a0making\u00a0the\u00a0problem\u00a0larger\u00a0as\u00a0time\u00a0goes\u00a0on. While these issues can be resolved with good programming practices, such as diligently carrying out code reviews, these measures are not\u00a0perfect.</p>\n<p>Browser extensions, however, has the added advantage of being a separate service, while still operating on the same site. With browser extensions, we can be assured that users of our sites will not be impacted by development and maintenance of internal\u00a0tools.</p>\n<h3>2. It is migration-proof</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*0K2DN15R_yXgpJBsa9UdtQ.png\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/vectors/penguin-migrate-berlin-bird-fly-154747/\">pixabay</a></figcaption></figure><p><a href=\"https://en.wikipedia.org/wiki/Service-oriented_architecture\">Service-Oriented Architecture (SOA)</a> advocates for services representing functionalities that are decoupled from each other, yet cohesively integrating with each other. This also means that any service in the product mesh could be migrated as long as the new services respect the same API contracts.</p>\n<p>Imagine that you need to redesign certain pages of your web service, or rewrite your web application for reasons like implementing as an app-shell architecture. Or maybe you\u2019re just modernising a legacy application. If your internal tools are coupled with your web application, you will have a field day migrating features that doesn\u2019t impact the majority of your users, and possibly bringing back bugs citing from the first reason\u00a0above.</p>\n<p>If you have implemented your internal tools with a browser extension, you\u2019ll find that the problems above are instantly resolved. Aside from hostname changes and targeted placement of UI elements from the extension\u00a0in\u00a0the\u00a0site, there isn\u2019t a need to rebuild any features with the ongoing migration.</p>\n<h3>3. Simple to execute for simple\u00a0stuff</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/736/1*WeJKbJuMoU--cinGR0ByoA.jpeg\"><figcaption>Original image, source: <a href=\"https://commons.wikimedia.org/wiki/File:Keep_it_Simple_(3340381990).jpg\">wikimedia</a></figcaption></figure><p>One day, your manager might come by and <a href=\"https://blog.viki.com/more-than-a-shower-thought-chrome-browser-extensions-instead-of-frontend-web-services-6d1c4d81a136\">ask you to add an \u201cedit\u201d link</a> in some pages. While it is certainly an achievable task, if your service has not been previously scoped to recognize user roles or staff status, this may lead to a rolling list of sub tasks\u00a0that\u00a0can\u00a0take\u00a0weeks\u00a0of\u00a0man\u00a0hours\u00a0to\u00a0complete.</p>\n<p>For such a simple objective, we can reduce the work done with a private browser extension. This can be done with adding the option in the right click context menu on all applicable pages (based on validating the page\u2019s URL), which can be done in a single afternoon.</p>\n<h3>4. Low barriers of entry to development</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1017/1*KbIPIptIcfmgK_ojW9jFuw.png\"><figcaption>Modified image, source:\u00a0<a href=\"https://pixabay.com/illustrations/achievement-across-advantage-703442/\">pixabay</a></figcaption></figure><p>My experience with Chrome extensions is that you only need knowledge of JavaScript to make something basic. Other knowledge like HTML and CSS will let you use other mechanisms like a <a href=\"https://developer.chrome.com/extensions/browserAction\">popup</a>, but not always necessary.</p>\n<p>Any web developer should have no problem with developing for Chrome extensions. For more advanced development\u00a0and\u00a0synchronising\u00a0team\u00a0practices, you can turn to using libraries like React as\u00a0well.</p>\n<p>Firefox offers a similar outlook in easy development with their\u00a0<a href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons\">add-ons</a>.</p>\n<h3>5. Low to no\u00a0cost</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*4ZVBeFO5QaEBLA05fMI9Aw.jpeg\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/illustrations/money-finance-cash-coins-3269654/\">pixabay</a></figcaption></figure><p>For hosting any web service, that will be costs incurred as long as a server is required to render the views. In the case of browser extensions, executing the extensions is free as computation is done on client-side (browser).</p>\n<p><a href=\"https://developer.chrome.com/apps/external_extensions\">Chrome</a> offers its own ways to install private extensions from local files. If you can afford to spend a bit (5 USD), you can make the distribution process simpler for Chrome. This is done by publishing as an unlisted or a private application (instructions <a href=\"https://support.google.com/chrome/a/answer/2714278\">here</a>).</p>\n<p>While development costs in time and money are still present, the setup is entirely server-less, reducing overall time spent on maintaining the service that could have been incurred from the infrastructure requirements.</p>\n<h3><strong>Extra. Get to learn something new at\u00a0work</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/0*d60k0mOu04bWV7YE.jpg\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/illustrations/education-learning-school-educate-919895/\">pixabay</a></figcaption></figure><p>Not so often you get to ask your boss to support your learning endeavors by justifying it with the reasons above. For me, I did this first in my own time to prove that it was worth investing into, and got it approved as a formal process at work. I even got a teammate to join me with adding more features.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*SBtJRUaWGxH_okYNWOHRZA.jpeg\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/photos/business-the-next-step-next-success-4241792/\">pixabay</a></figcaption></figure><p>Beyond the simple examples discussed above, there are many useful features that can be achieved with a browser extension. One that I had in mind, was a QA feature to record UI inconsistencies and bugs and generate tasks in out task management tool. The sky\u2019s the limit\u00a0here.</p>\n<p>That\u2019s it. I hope that the above gave a different perspective on how private browser extensions can be harnessed as a service for internal tools, developed as an easier and less costly\u00a0alternative.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8a9a88fa452d\" width=\"1\" height=\"1\">\n","content":"\n<h3>Content\u00a0Management\u00a0tools, implemented with private browser extensions</h3>\n<h4>Making internal content management tools easier to\u00a0implement, and breaking lesser things in the\u00a0process.</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Ugcb2Izz4bQkfgOc7Cksfw.png\"><figcaption>Modified image, source:\u00a0<a href=\"https://svgsilh.com/image/34443.html\">svgsilh</a></figcaption></figure><p>If you have built some web service before, or in the process of planning to build one, you might have thought about putting in internal features on selected pages. Perhaps an <em>\u201cEdit Post\u201d</em> right-click menu option if you\u2019re building a news site, or <em>\u201cList Sellers\u201d</em> button if you\u2019re building an e-commerce site, both which could be linked to an another web\u00a0service.</p>\n<p>Developing those internal features is certainly\u00a0doable. But for features that cater to internal\u00a0users,\u00a0we\u00a0may\u00a0be\u00a0looking\u00a0at\u00a01%\u00a0or an\u00a0even\u00a0lesser proportion\u00a0of\u00a0the\u00a0audience. Implementation becomes costly in intangible ways, such as impacting regular users if bugs arise from these internal features.</p>\n<p>Here\u2019s my take on why it\u2019s better to implement these internal tools with private browser extensions.</p>\n<h3>1. We can incrementally add internal features, without impacting main user\u00a0groups</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*l7owKKVneKskw3RrmzxvQw.png\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/illustrations/under-construction-construction-sign-2408062/\">pixabay</a></figcaption></figure><p>Let\u2019s say we add a new process to our product site that is only meant for our staff. It raises some questions:</p>\n<blockquote>\u201cWhat if parts of the process are accidentally leaked to our clients? Or what if this process broke the page and affects all our\u00a0users?\u201d</blockquote>\n<p>In the\u00a0above\u00a0questions, we highlight the risk of creating bugs that are part and parcel of feature releases. For the observed\u00a0bugs, we have to spend resources to fix the tools, and rollback if necessary.</p>\n<p>However, the surface area for bugs expands as we add more internal tools to our web service,\u00a0making\u00a0the\u00a0problem\u00a0larger\u00a0as\u00a0time\u00a0goes\u00a0on. While these issues can be resolved with good programming practices, such as diligently carrying out code reviews, these measures are not\u00a0perfect.</p>\n<p>Browser extensions, however, has the added advantage of being a separate service, while still operating on the same site. With browser extensions, we can be assured that users of our sites will not be impacted by development and maintenance of internal\u00a0tools.</p>\n<h3>2. It is migration-proof</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*0K2DN15R_yXgpJBsa9UdtQ.png\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/vectors/penguin-migrate-berlin-bird-fly-154747/\">pixabay</a></figcaption></figure><p><a href=\"https://en.wikipedia.org/wiki/Service-oriented_architecture\">Service-Oriented Architecture (SOA)</a> advocates for services representing functionalities that are decoupled from each other, yet cohesively integrating with each other. This also means that any service in the product mesh could be migrated as long as the new services respect the same API contracts.</p>\n<p>Imagine that you need to redesign certain pages of your web service, or rewrite your web application for reasons like implementing as an app-shell architecture. Or maybe you\u2019re just modernising a legacy application. If your internal tools are coupled with your web application, you will have a field day migrating features that doesn\u2019t impact the majority of your users, and possibly bringing back bugs citing from the first reason\u00a0above.</p>\n<p>If you have implemented your internal tools with a browser extension, you\u2019ll find that the problems above are instantly resolved. Aside from hostname changes and targeted placement of UI elements from the extension\u00a0in\u00a0the\u00a0site, there isn\u2019t a need to rebuild any features with the ongoing migration.</p>\n<h3>3. Simple to execute for simple\u00a0stuff</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/736/1*WeJKbJuMoU--cinGR0ByoA.jpeg\"><figcaption>Original image, source: <a href=\"https://commons.wikimedia.org/wiki/File:Keep_it_Simple_(3340381990).jpg\">wikimedia</a></figcaption></figure><p>One day, your manager might come by and <a href=\"https://blog.viki.com/more-than-a-shower-thought-chrome-browser-extensions-instead-of-frontend-web-services-6d1c4d81a136\">ask you to add an \u201cedit\u201d link</a> in some pages. While it is certainly an achievable task, if your service has not been previously scoped to recognize user roles or staff status, this may lead to a rolling list of sub tasks\u00a0that\u00a0can\u00a0take\u00a0weeks\u00a0of\u00a0man\u00a0hours\u00a0to\u00a0complete.</p>\n<p>For such a simple objective, we can reduce the work done with a private browser extension. This can be done with adding the option in the right click context menu on all applicable pages (based on validating the page\u2019s URL), which can be done in a single afternoon.</p>\n<h3>4. Low barriers of entry to development</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1017/1*KbIPIptIcfmgK_ojW9jFuw.png\"><figcaption>Modified image, source:\u00a0<a href=\"https://pixabay.com/illustrations/achievement-across-advantage-703442/\">pixabay</a></figcaption></figure><p>My experience with Chrome extensions is that you only need knowledge of JavaScript to make something basic. Other knowledge like HTML and CSS will let you use other mechanisms like a <a href=\"https://developer.chrome.com/extensions/browserAction\">popup</a>, but not always necessary.</p>\n<p>Any web developer should have no problem with developing for Chrome extensions. For more advanced development\u00a0and\u00a0synchronising\u00a0team\u00a0practices, you can turn to using libraries like React as\u00a0well.</p>\n<p>Firefox offers a similar outlook in easy development with their\u00a0<a href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons\">add-ons</a>.</p>\n<h3>5. Low to no\u00a0cost</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*4ZVBeFO5QaEBLA05fMI9Aw.jpeg\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/illustrations/money-finance-cash-coins-3269654/\">pixabay</a></figcaption></figure><p>For hosting any web service, that will be costs incurred as long as a server is required to render the views. In the case of browser extensions, executing the extensions is free as computation is done on client-side (browser).</p>\n<p><a href=\"https://developer.chrome.com/apps/external_extensions\">Chrome</a> offers its own ways to install private extensions from local files. If you can afford to spend a bit (5 USD), you can make the distribution process simpler for Chrome. This is done by publishing as an unlisted or a private application (instructions <a href=\"https://support.google.com/chrome/a/answer/2714278\">here</a>).</p>\n<p>While development costs in time and money are still present, the setup is entirely server-less, reducing overall time spent on maintaining the service that could have been incurred from the infrastructure requirements.</p>\n<h3><strong>Extra. Get to learn something new at\u00a0work</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/0*d60k0mOu04bWV7YE.jpg\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/illustrations/education-learning-school-educate-919895/\">pixabay</a></figcaption></figure><p>Not so often you get to ask your boss to support your learning endeavors by justifying it with the reasons above. For me, I did this first in my own time to prove that it was worth investing into, and got it approved as a formal process at work. I even got a teammate to join me with adding more features.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*SBtJRUaWGxH_okYNWOHRZA.jpeg\"><figcaption>Original image, source:\u00a0<a href=\"https://pixabay.com/photos/business-the-next-step-next-success-4241792/\">pixabay</a></figcaption></figure><p>Beyond the simple examples discussed above, there are many useful features that can be achieved with a browser extension. One that I had in mind, was a QA feature to record UI inconsistencies and bugs and generate tasks in out task management tool. The sky\u2019s the limit\u00a0here.</p>\n<p>That\u2019s it. I hope that the above gave a different perspective on how private browser extensions can be harnessed as a service for internal tools, developed as an easier and less costly\u00a0alternative.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8a9a88fa452d\" width=\"1\" height=\"1\">\n","enclosure":{},"categories":["programming","self-improvement","technology","browsers","web-development"]},{"title":"Collaborative Email Templates with Google Drive and Gmail","pubDate":"2019-04-16 16:11:06","link":"https://medium.com/@weiyuan.liu/collaborative-email-templates-with-google-drive-and-gmail-c6cceb1b1ea7?source=rss-669623997d13------2","guid":"https://medium.com/p/c6cceb1b1ea7","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/430/1*cdgT6-rTVYsBVdc6UXbLpA.png","description":"\n<h3>Collaborative Email Templates with Apps Scripts, Drive and\u00a0Gmail</h3>\n<h4>Templating logic with Apps Scripts and Google Docs, and automating emails with\u00a0Gmail</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/430/1*cdgT6-rTVYsBVdc6UXbLpA.png\"></figure><p>Some time ago, I was discussing with another colleague of mine on rewriting some onboarding documentation for new engineers. You see, our teams had just merged due to some organisational changes. I wouldn\u2019t go too deep into the irrelevant details, but just that both teams used different systems, and one team communicated with an onboarding email linked to each of the documentation, while the other communicated the links to the documentation offline. Documentation for both teams resided in different systems, and was outdated in some areas as\u00a0well.</p>\n<p><strong>Preface\u200a\u2014\u200aMaking documentation collaborative via Google\u00a0Docs</strong></p>\n<p>We first solved the outdated and fragmented documentation issue by migrating and reviewing the new content with Google Docs. This doubled up as a collaborative solution for members of both teams\u00a0in\u00a0updates. With the updated documentation at hand for the newly formed team, we decided to follow the methodology of delivery by sending the links via\u00a0email.</p>\n<p>Here\u2019s where our main problem came in, the email template was essentially a template where the names of the manager, buddy and the newcomer had to be filled in at various points\u200a\u2014\u200aand this was done manually. From experience, manual solutions are often troublesome. They are easily susceptible to human error (e.g. forget to replace name in template, or using the wrong name). Being cautious and checking the email repeatedly before sending is a sound solution, but that could take up more time which could be used for more productive tasks at\u00a0work.</p>\n<p>We could turn to industry solutions such as Sendgrid or Iterable, but where\u2019s the fun in that\u00a0:p (plus for a small internal use case it doesn\u2019t justify the\u00a0cost)</p>\n<p><strong>Automating a solution?</strong></p>\n<p>Since we were using Google Docs, this gave rise to the potential for scripting a solution using Apps Scripts. But like the average every-man, my experience with Google Docs was just to the point of using it like Microsoft Word. I had experience with Excel formulas, but it was not at all related to the scripting interface.</p>\n<p>By a stroke of luck, I had some Qwiklabs credits lying around, and there were some courses that were useful in showing how Apps Scripts works. Fret not if you don\u2019t have any Qwiklabs credits, as the instructions for the courses linked below (at the end of the next chapter) are public. As long as you have a Google Account, you could use it in place of the generated student account for the lab. There are also public tutorials that can get you started on learning to develop in Apps\u00a0Scripts.</p>\n<p><strong>How Apps Scripts work (Hello\u00a0World!)</strong></p>\n<p>To start off, we need to create or use a existing Document on Google Docs. You\u2019ll find the script editor as seen below from the menu: Tools &gt; Script editor\u00a0.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/684/1*LHF8w1_gt1k_f5p9i3_Pxw.jpeg\"></figure><p>Following the selection of the \u201cScript editor\u201d menu item, you should see a editor interface with a empty myFunction() implemented in code. Before we go any further, you may have realised that the code is saved with the extension\u00a0.gs\u00a0, which stands for Google Apps Scripts. Fortunately, it\u2019s not entirely a new language to learn, and that it is based off <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript\">JavaScript</a>, which is already used in browser clients and back-end services\u00a0today.</p>\n<blockquote>Google Apps Script is a rapid application development platform that makes it fast and easy to create business applications that integrate with G Suite.<strong> You write code in JavaScript and have access to built-in libraries</strong> for favorite G Suite applications like Gmail, Calendar, Drive, and\u00a0more.</blockquote>\n<p><strong><em>- above excerpt taken from\u00a0</em></strong><a href=\"https://developers.google.com/apps-script/overview\"><strong><em>here</em></strong></a></p>\n<a href=\"https://medium.com/media/125e9055e071c25e7d744a24ccba84f3/href\">https://medium.com/media/125e9055e071c25e7d744a24ccba84f3/href</a><p>Next, to test out how Apps Scripts work, you can take the above the code and apply it within your editor. Click the play button (labelled as \u201cRun\u201d) as seen in the following image:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/468/1*3zyFiXJK2f5ftwTKUfJ8fQ.png\"></figure><p>Switch back to your Document, where you should see the following modal:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/605/1*_nkS21-U4PfLuQSkCCwE3A.png\"></figure><p>If you have followed the above steps so far, you should have a basic understanding on how Apps Scripts interact with your Document. If you\u2019re interested, here\u2019s the content as shared earlier on for the Qwiklabs tutorial and the tutorials provided by\u00a0Google.</p>\n<ul>\n<li><a href=\"https://www.qwiklabs.com/focuses/3688?parent=catalog\">Qwiklabs tutorial</a></li>\n<li><a href=\"https://developers.google.com/apps-script/articles/tutorials\">Google tutorials</a></li>\n</ul>\n<p><strong>Collaborative Email Templates</strong></p>\n<p>There\u2019s a lot that can be done using Apps Scripts in achieving our goals here. The entirety of the solution will be done in the script editor so that we can see the output immediately. Each part of the solution is detailed as follows, and the source code can be found\u00a0<a href=\"https://github.com/Weiyuan-Lane/GoogleDocsEmail\">here</a>:</p>\n<ul>\n<li>Event Hooks</li>\n<li>Sidebar</li>\n<li>DocumentApp API and\u00a0regex</li>\n<li>Render Document as\u00a0HTML</li>\n<li>Mailing API</li>\n</ul>\n<p><strong>Event Hooks (documentation </strong><a href=\"https://developers.google.com/apps-script/guides/triggers/#getting_started\"><strong>here</strong></a><strong>)</strong></p>\n<p>To start off, we can implement event hooks onInstall and onOpen\u00a0, for adding the add-on menu items to trigger the Sidebar in the next\u00a0part.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/521/1*qzKgK_712SLw5zt5Bi_DWQ.png\"></figure><p>After saving the code and refreshing the Document webview, I am able to see the following menu items under\u00a0Add-ons:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/518/1*zij3kBeQ-TmODLFMjHKOsw.png\"></figure><p>Clicking it triggers the alert method as expected.</p>\n<p><strong>Sidebar (guide\u00a0</strong><a href=\"https://developers.google.com/apps-script/guides/dialogs#custom_sidebars\"><strong>here</strong></a><strong>)</strong></p>\n<p>To implement the sidebar, we need to first create a HTML file ( which can be created from the script editor from File &gt; New &gt; HTML file ). I named mine as sidebar.html\u00a0, but you can named it however you like as long as it is referenced properly in\u00a0code.</p>\n<p>Next, we need to reference the HTML file to display within the sidebar. I\u2019ve used the HtmlService API to reference sidebar.html from above, to create the output for the\u00a0sidebar.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/616/1*mmLEC905r7mMC2yzeygUkQ.png\"></figure><p>Refreshing the Document again, we should see the following sidebar with the provided title and HTML content once we select the\u00a0add-on.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/621/1*Mf0biW7siNJogmxy5zd90w.png\"></figure><p>More content is added in the HTML file eventually, such as CSS and JavaScript, and even loading third party libraries like jQuery. One important note is the communication between front-end JavaScript in the\u00a0.html file and back-end Google Apps Scripts in the\u00a0.gs file. From the front-end JavaScript, we can use the google API - google.script.run - to invoke the back-end methods with the intended parameters directly. More information on this communication <a href=\"https://developers.google.com/apps-script/guides/html/reference/run\">here</a>.</p>\n<p><strong>DocumentApp API and regex (DocumentApp documentation </strong><a href=\"https://developers.google.com/apps-script/reference/document/document-app\"><strong>here</strong></a><strong>)</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/578/1*kPssvNa9lja9YocS5Dv56Q.png\"></figure><p>With the sidebar implemented, I proceeded to add the back-end function to read from the Document and parse the variables from the regular text. The outcome of the computation was to be sent back to the front-end where the information is displayed on the sidebar for user\u00a0input.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/305/1*ZEU4G40fqkzBpYJIJDOmEw.png\"></figure><p><strong>Render Document as HTML (Adapted from discussion </strong><a href=\"https://stackoverflow.com/questions/14663852/get-google-document-as-html\"><strong>here</strong></a><strong>)</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/873/1*EnlaYXnNd5aVvpsdyRFauQ.png\"></figure><p>Rendering Google Docs as HTML was interesting, as there was no direct API in DocumentApp that could do this. From a stackoverflow solution, it was suggested that we use UrlFetchApp to invoke a url that converts the Document into HTML. By implementing this, and running the template rendering logic with the user-provided variables, I am able to obtain a method that can be used for both previewing the email for checking and sending the email with HTML\u00a0output.</p>\n<p><strong>Mailing API (MailApp Documentation </strong><a href=\"https://developers.google.com/apps-script/reference/mail/mail-app\"><strong>here</strong></a><strong>)</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/434/1*_nGZdAheC1tjXWe3oMiPRQ.png\"></figure><p>With all the above accomplished, invoking the Mailing API is a walk in the park. By supplying the html\u00a0body, subject, and the email recipients, we should be able to send the email very easily. Do take note not to exceed the limit\u00a0<a href=\"https://developers.google.com/gmail/api/v1/reference/quota\">here</a>.</p>\n<p><strong>Outcome</strong></p>\n<p>The end result is as seen here, fast template rendering with no worries. Source code as seen\u00a0<a href=\"https://github.com/Weiyuan-Lane/GoogleDocsEmail\">here</a>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6Wwt6CLiiZuD2qb5SAsPlw.gif\"></figure><p>While not discussed above, I\u2019ve added some caching logic for some of the content, such as the template character, and the recipients\u2019 emails, as these content tend to not change. This optimisation allows the user to send their emails out even more\u00a0quickly.</p>\n<p>Another suggestion by a colleague at work was to use Google Sheets to bypass the UI and send the email in large batches. This could be a useful improvement for a future use\u00a0case.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c6cceb1b1ea7\" width=\"1\" height=\"1\">\n","content":"\n<h3>Collaborative Email Templates with Apps Scripts, Drive and\u00a0Gmail</h3>\n<h4>Templating logic with Apps Scripts and Google Docs, and automating emails with\u00a0Gmail</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/430/1*cdgT6-rTVYsBVdc6UXbLpA.png\"></figure><p>Some time ago, I was discussing with another colleague of mine on rewriting some onboarding documentation for new engineers. You see, our teams had just merged due to some organisational changes. I wouldn\u2019t go too deep into the irrelevant details, but just that both teams used different systems, and one team communicated with an onboarding email linked to each of the documentation, while the other communicated the links to the documentation offline. Documentation for both teams resided in different systems, and was outdated in some areas as\u00a0well.</p>\n<p><strong>Preface\u200a\u2014\u200aMaking documentation collaborative via Google\u00a0Docs</strong></p>\n<p>We first solved the outdated and fragmented documentation issue by migrating and reviewing the new content with Google Docs. This doubled up as a collaborative solution for members of both teams\u00a0in\u00a0updates. With the updated documentation at hand for the newly formed team, we decided to follow the methodology of delivery by sending the links via\u00a0email.</p>\n<p>Here\u2019s where our main problem came in, the email template was essentially a template where the names of the manager, buddy and the newcomer had to be filled in at various points\u200a\u2014\u200aand this was done manually. From experience, manual solutions are often troublesome. They are easily susceptible to human error (e.g. forget to replace name in template, or using the wrong name). Being cautious and checking the email repeatedly before sending is a sound solution, but that could take up more time which could be used for more productive tasks at\u00a0work.</p>\n<p>We could turn to industry solutions such as Sendgrid or Iterable, but where\u2019s the fun in that\u00a0:p (plus for a small internal use case it doesn\u2019t justify the\u00a0cost)</p>\n<p><strong>Automating a solution?</strong></p>\n<p>Since we were using Google Docs, this gave rise to the potential for scripting a solution using Apps Scripts. But like the average every-man, my experience with Google Docs was just to the point of using it like Microsoft Word. I had experience with Excel formulas, but it was not at all related to the scripting interface.</p>\n<p>By a stroke of luck, I had some Qwiklabs credits lying around, and there were some courses that were useful in showing how Apps Scripts works. Fret not if you don\u2019t have any Qwiklabs credits, as the instructions for the courses linked below (at the end of the next chapter) are public. As long as you have a Google Account, you could use it in place of the generated student account for the lab. There are also public tutorials that can get you started on learning to develop in Apps\u00a0Scripts.</p>\n<p><strong>How Apps Scripts work (Hello\u00a0World!)</strong></p>\n<p>To start off, we need to create or use a existing Document on Google Docs. You\u2019ll find the script editor as seen below from the menu: Tools &gt; Script editor\u00a0.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/684/1*LHF8w1_gt1k_f5p9i3_Pxw.jpeg\"></figure><p>Following the selection of the \u201cScript editor\u201d menu item, you should see a editor interface with a empty myFunction() implemented in code. Before we go any further, you may have realised that the code is saved with the extension\u00a0.gs\u00a0, which stands for Google Apps Scripts. Fortunately, it\u2019s not entirely a new language to learn, and that it is based off <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript\">JavaScript</a>, which is already used in browser clients and back-end services\u00a0today.</p>\n<blockquote>Google Apps Script is a rapid application development platform that makes it fast and easy to create business applications that integrate with G Suite.<strong> You write code in JavaScript and have access to built-in libraries</strong> for favorite G Suite applications like Gmail, Calendar, Drive, and\u00a0more.</blockquote>\n<p><strong><em>- above excerpt taken from\u00a0</em></strong><a href=\"https://developers.google.com/apps-script/overview\"><strong><em>here</em></strong></a></p>\n<a href=\"https://medium.com/media/125e9055e071c25e7d744a24ccba84f3/href\">https://medium.com/media/125e9055e071c25e7d744a24ccba84f3/href</a><p>Next, to test out how Apps Scripts work, you can take the above the code and apply it within your editor. Click the play button (labelled as \u201cRun\u201d) as seen in the following image:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/468/1*3zyFiXJK2f5ftwTKUfJ8fQ.png\"></figure><p>Switch back to your Document, where you should see the following modal:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/605/1*_nkS21-U4PfLuQSkCCwE3A.png\"></figure><p>If you have followed the above steps so far, you should have a basic understanding on how Apps Scripts interact with your Document. If you\u2019re interested, here\u2019s the content as shared earlier on for the Qwiklabs tutorial and the tutorials provided by\u00a0Google.</p>\n<ul>\n<li><a href=\"https://www.qwiklabs.com/focuses/3688?parent=catalog\">Qwiklabs tutorial</a></li>\n<li><a href=\"https://developers.google.com/apps-script/articles/tutorials\">Google tutorials</a></li>\n</ul>\n<p><strong>Collaborative Email Templates</strong></p>\n<p>There\u2019s a lot that can be done using Apps Scripts in achieving our goals here. The entirety of the solution will be done in the script editor so that we can see the output immediately. Each part of the solution is detailed as follows, and the source code can be found\u00a0<a href=\"https://github.com/Weiyuan-Lane/GoogleDocsEmail\">here</a>:</p>\n<ul>\n<li>Event Hooks</li>\n<li>Sidebar</li>\n<li>DocumentApp API and\u00a0regex</li>\n<li>Render Document as\u00a0HTML</li>\n<li>Mailing API</li>\n</ul>\n<p><strong>Event Hooks (documentation </strong><a href=\"https://developers.google.com/apps-script/guides/triggers/#getting_started\"><strong>here</strong></a><strong>)</strong></p>\n<p>To start off, we can implement event hooks onInstall and onOpen\u00a0, for adding the add-on menu items to trigger the Sidebar in the next\u00a0part.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/521/1*qzKgK_712SLw5zt5Bi_DWQ.png\"></figure><p>After saving the code and refreshing the Document webview, I am able to see the following menu items under\u00a0Add-ons:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/518/1*zij3kBeQ-TmODLFMjHKOsw.png\"></figure><p>Clicking it triggers the alert method as expected.</p>\n<p><strong>Sidebar (guide\u00a0</strong><a href=\"https://developers.google.com/apps-script/guides/dialogs#custom_sidebars\"><strong>here</strong></a><strong>)</strong></p>\n<p>To implement the sidebar, we need to first create a HTML file ( which can be created from the script editor from File &gt; New &gt; HTML file ). I named mine as sidebar.html\u00a0, but you can named it however you like as long as it is referenced properly in\u00a0code.</p>\n<p>Next, we need to reference the HTML file to display within the sidebar. I\u2019ve used the HtmlService API to reference sidebar.html from above, to create the output for the\u00a0sidebar.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/616/1*mmLEC905r7mMC2yzeygUkQ.png\"></figure><p>Refreshing the Document again, we should see the following sidebar with the provided title and HTML content once we select the\u00a0add-on.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/621/1*Mf0biW7siNJogmxy5zd90w.png\"></figure><p>More content is added in the HTML file eventually, such as CSS and JavaScript, and even loading third party libraries like jQuery. One important note is the communication between front-end JavaScript in the\u00a0.html file and back-end Google Apps Scripts in the\u00a0.gs file. From the front-end JavaScript, we can use the google API - google.script.run - to invoke the back-end methods with the intended parameters directly. More information on this communication <a href=\"https://developers.google.com/apps-script/guides/html/reference/run\">here</a>.</p>\n<p><strong>DocumentApp API and regex (DocumentApp documentation </strong><a href=\"https://developers.google.com/apps-script/reference/document/document-app\"><strong>here</strong></a><strong>)</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/578/1*kPssvNa9lja9YocS5Dv56Q.png\"></figure><p>With the sidebar implemented, I proceeded to add the back-end function to read from the Document and parse the variables from the regular text. The outcome of the computation was to be sent back to the front-end where the information is displayed on the sidebar for user\u00a0input.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/305/1*ZEU4G40fqkzBpYJIJDOmEw.png\"></figure><p><strong>Render Document as HTML (Adapted from discussion </strong><a href=\"https://stackoverflow.com/questions/14663852/get-google-document-as-html\"><strong>here</strong></a><strong>)</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/873/1*EnlaYXnNd5aVvpsdyRFauQ.png\"></figure><p>Rendering Google Docs as HTML was interesting, as there was no direct API in DocumentApp that could do this. From a stackoverflow solution, it was suggested that we use UrlFetchApp to invoke a url that converts the Document into HTML. By implementing this, and running the template rendering logic with the user-provided variables, I am able to obtain a method that can be used for both previewing the email for checking and sending the email with HTML\u00a0output.</p>\n<p><strong>Mailing API (MailApp Documentation </strong><a href=\"https://developers.google.com/apps-script/reference/mail/mail-app\"><strong>here</strong></a><strong>)</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/434/1*_nGZdAheC1tjXWe3oMiPRQ.png\"></figure><p>With all the above accomplished, invoking the Mailing API is a walk in the park. By supplying the html\u00a0body, subject, and the email recipients, we should be able to send the email very easily. Do take note not to exceed the limit\u00a0<a href=\"https://developers.google.com/gmail/api/v1/reference/quota\">here</a>.</p>\n<p><strong>Outcome</strong></p>\n<p>The end result is as seen here, fast template rendering with no worries. Source code as seen\u00a0<a href=\"https://github.com/Weiyuan-Lane/GoogleDocsEmail\">here</a>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6Wwt6CLiiZuD2qb5SAsPlw.gif\"></figure><p>While not discussed above, I\u2019ve added some caching logic for some of the content, such as the template character, and the recipients\u2019 emails, as these content tend to not change. This optimisation allows the user to send their emails out even more\u00a0quickly.</p>\n<p>Another suggestion by a colleague at work was to use Google Sheets to bypass the UI and send the email in large batches. This could be a useful improvement for a future use\u00a0case.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c6cceb1b1ea7\" width=\"1\" height=\"1\">\n","enclosure":{},"categories":["google-drive","web-development","self-improvement","technology","programming"]},{"title":"Modernizing Soompi\u200a\u2014\u200aPart 3","pubDate":"2019-04-08 08:34:47","link":"https://blog.viki.com/modernising-soompi-part-3-14b1483b5c16?source=rss-669623997d13------2","guid":"https://medium.com/p/14b1483b5c16","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/794/1*gQhIUB05nCDeBUrhBxFiqA.jpeg","description":"\n<h3><strong>Modernizing Soompi\u200a\u2014\u200aPart\u00a03</strong></h3>\n<h4><strong>Establishing a new standard for front-end services</strong></h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/794/1*gQhIUB05nCDeBUrhBxFiqA.jpeg\"></figure><p>In an idealistic world, rewriting applications would be the optimal solution for most programming problems. Legacy issues? Rewrite them away! Lost context of what the service entails? Rewrite your understanding in! However, such a naive approach to solving such problems is not always the right way, given real world limitations such as fixed amounts of time to complete projects, and deriving revenue (in turn paying our salaries!) to sustain the project\u2019s momentum.</p>\n<p>One might then ask, \u201cWhy should we embark on this project to rewrite our WordPress service for the front-end\u00a0?\u201d</p>\n<p>The result for us, summarized in one line: \u201cEstablishing a new standard for front-end services\u201d</p>\n<p><strong>Establishing new standards</strong></p>\n<p>\u201cEstablishing a new standard\u201d here suggests a pivotal change in our organisation on how we looked at our front-end applications, and causing substantial reform to invoke ripples of change for the better. At the point of proposing the rewrite for the application, teams at Viki had already tried out different tools. One such tool was ReactJS, which was used in <a href=\"https://www.viki.com/\">Viki\u2019s main site</a>, and to a larger extent, <a href=\"https://www.soompi.com/awards\">Soompi Awards</a>, a branch-off site from Soompi that used React-router to create a Single Page Application (SPA).</p>\n<p>Over time, we found that React was better than other tools we used for front-end components. For example, one benefit we found was that React could be used to create components with very little coupling to the core tech stack. This potential for re-usability was better than our usage of CoffeeScript components, which was coupled to another of our Ruby on Rails (RoR) application.</p>\n<p>The above highlights the notion of building for re-usability, between unrelated services of the immediate team and by extension, the company. By building for more patterns in the same vein as the above, we strive towards building a \u201cboilerplate\u201d of tools that we can use for all front-end services. This benefits the team greatly as context switching is minimized when team members are reassigned between different projects.</p>\n<p>The mentality extended beyond the tech stack\u200a\u2014\u200awe looked at the accumulated performance enhancements that we made in different services, and applied them in this rewrite to solve performance issues we found in our current WordPress application. Infrastructure and scaling was also based on the experiences gained from building micro services using AppEngine, where we observed the benefit in simplifying our team\u2019s core infrastructure responsibilities and skills, allowing us to focus on front-end related feature-sets.</p>\n<h3><strong>Client View as a standard\u200a\u2014\u200aSPA + Isomorphic</strong></h3>\n<p>We started our rewrite with exploring the rendered view for the users. Our WordPress application render content mostly on the server side. This leaves the client side as a \u201cuntapped mine of precious resources\u201d. To that end, we decide to look into the principles of <a href=\"https://developers.google.com/web/fundamentals/architecture/app-shell\">app shell architecture</a>, or the more precise approach, SPA.</p>\n<p>SPA is defined as follows\u200a\u2014\u200aan application that revises part of the view for various combinations of \u201ccontext\u201d. \u201cContext\u201d here can refer to Fully Qualified Domain Name (FQDN), path and state (such as cookies for session usage or GET parameters that denote options for rendering the current\u00a0page).</p>\n<p>For this rewrite, the SPA would translate to implementing logic on the client side that revises the web page when the page\u2019s context changes. This is in stark contrast to our WordPress application, where each HTTP request formed from the same \u201ccontext\u201d result in a newly rendered view from the\u00a0server.</p>\n<p>One might ask: \u201cHow does moving the rendering responsibilities from the server-side to the client-side make for a better user experience?\u201d The answer to that lies in optimizing the behavior of browsers in loading client-facing web pages. Traditionally, loading a page returns a HTML response from the server-side of a service, which in turn provides the instructions for the browser to parse. This is followed by downloading various assets (JavaScript, CSS and font) and creating the DOM elements with the associated event callbacks (not necessary in that\u00a0order).</p>\n<p>For a SPA, loading the first page does the same thing, but subsequent pages do not incur the overhead of reloading and parsing assets as well as the full view. This results in better latency for loading each page due to lesser assets to download, and thus higher levels of user satisfaction, and lower bandwidth costs to our services.</p>\n<p><strong>Implementation</strong></p>\n<p>We used <a href=\"https://github.com/ReactTraining/react-router\">React-Router</a> to create our own SPA. This was done by referencing the current context of the page to different route-centric components. For example, a URL with the path as \u201c/\u201d will utilize the home route component, while a URL with the path of \u201c/article/:id\u201d will utilize the article route component. Changes in the URL through clicking on internal links within the site will trigger an update to the route component currently used.</p>\n<p>It is important to note that the different common functionalities of each page was not coupled to the route components, in order to maximize re-usability and ease of future updates to these functionalities. These functionalities can be divided to two areas\u200a\u2014\u200apage level components and utility level components.</p>\n<a href=\"https://medium.com/media/1eee4b691b830ef1fedfc2611a042f54/href\">https://medium.com/media/1eee4b691b830ef1fedfc2611a042f54/href</a><p>Page level components are as observed in Fig 1 above, through the \u201cNavbar\u201d element. It is safe to assume that the Navbar does not differ between route components for a single web application. Hence, this type of functionalities should be rendered independently from the routes, as seen in \u201crenderRoutes\u201d above.</p>\n<p>Utility level components are the core building blocks of the application. One such example is an \u201cInput\u201d component that gives a warning if the inputted characters exceed the limit passed in through the component\u2019s props. This component can be reused in creating a new user account, or writing a comment to an article. Hence it would be wise to decouple from the route, providing for reusable components within different route components or even page level components.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qKW_7EtLy61CP0gYCm4IcQ.jpeg\"><figcaption>Fig 2. App shell model (Shell component as seen on the left, while on the right it is rendered with\u00a0content)</figcaption></figure><p>In addition to the above, route-centric components must encompass the app shell model. This means that each route component is to be associated to a server-side route, which provides for a response (often JSON) that informs the corresponding component on what should be rendered. There can also be route independent identifiers, such as IDs within the URL (e.g. \u201c/article/1\u201d where \u201c1\u201d is the ID and \u201carticle\u201d references the article component), which are relayed within the request to the server-side for an individualized response. In Fig 2 above, we can see how this is represented, where a route can exist in different states until the content is retrieved.</p>\n<p><strong>SEO and Isopmorphism</strong></p>\n<p>However, a service implementing only a SPA would face problems with Search Engine Optimization (SEO). This is because the initial HTML output of the page only contains the \u201cshell\u201d of the SPA and no contents as observed in Fig 2 above. If any SEO related crawlers does not support the downloading, parsing and execution of JavaScript assets, crawling the page would not culminate to any meaningful search results from just the \u201cshell\u201d\u00a0alone.</p>\n<p>To solve this, we introduce the concept of server-side rendering for the initial load, and inherently creating an Isomorphic Application. A JavaScript engine (e.g. V8) is utilized on the server-side to do the exact same task as the client-side for the initial load. This allows us to preserves the app shell architecture required for subsequent context changes of the page. By virtue of rendering information within the initial load, we allow for bots to retrieve meaningful information that we want to couple with our search\u00a0results.</p>\n<h3><strong>Standardising the infrastructure - Google\u00a0Cloud</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*deKwicY2AmVIi3ty\"></figure><p>Let\u2019s review our infrastructure before we migrated over to <a href=\"https://cloud.google.com/appengine/\">Google App Engine</a>. Our WordPress application was hosted on several manually provisioned VMs (while containerized using Docker) from a cloud provider, gated behind a load balancer. Continuous Integration + Continuous Delivery (CICD) pipeline was done using a variant of Jenkins, with manual deployment using internally developed scripts. As for static assets, such as the compiled JS and CSS, were served from Amazon CloudFront as the Content Delivery Network\u00a0(CDN).</p>\n<p>When analyzing these components of our infrastructure, we observe that the last one was fully furnished by an external partner. Since the CDN component was already providing the benefits that we were looking for in the delivery of static assets, maintained and perfected in its own right, we didn\u2019t sought to work on improving this component. We focused on uplifting the infrastructure for deployment pipeline and application instead.</p>\n<p><strong>Part 1: CICD with Cloud Build, and IAM integrations</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*oDS181_KrUukUT2k\"><figcaption>Fig 3. From IAM menu of Cloud Console\u200a\u2014\u200acontrol permissions of what Cloud Build (service account) can do, such as deploying to App Engine and set 100% of traffic to new version for staging Cloud\u00a0Build.</figcaption></figure><p>For the CICD pipeline updates, we utilized <a href=\"https://cloud.google.com/cloud-build/\">Cloud Build</a>, primarily for a scalable CI experience. This was essential as our in-house CI was hosted on a single machine and would suffer from performance slowdowns or even disruptions when more end-users (developers) were utilizing it.</p>\n<p>Another plus point was that in using Cloud Build, we could leverage benefits for App Engine that was provided by the same partner. One crucial benefit was the usage of <a href=\"https://cloud.google.com/iam/\">Cloud IAM</a>, where we could utilize a privilege scheme to systemize workflow through service accounts used by different Google Cloud tools, as seen in Fig 3. This also meant that by virtue of being in the same ecosystem, there is no need to bake the credentials into our CICD pipeline for the service account that is performing the delivery or deployment.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*zn6-cA6Tf_92JcDC\"><figcaption>Fig 4. Custom role with assigned permissions for any newly on-boarded engineer</figcaption></figure><p>In addition to giving privileges to service accounts, we can also create custom roles for different privilege levels for each engineer. This allows for engineers to have sufficient view rights to monitor the application\u2019s health, but not direct deployment privileges so that checks and balances can be integrated into formal processes (such as enforcing automated tests before automated deployment or delivery) through our CICD pipeline. By employing this arrangement, we can minimize human error by ensuring that only service accounts can perform certain actions, such as deployment.</p>\n<p>In using Cloud Build, we also prepare different configurations for different environments, which are mapped to branch names via regex matching. Using shell scripts that we checked in, and web hook triggers to Cloud Build, developers could either order for a \u201cbuild-and-redeploy\u201d to staging, or \u201cbuild-and-deliver\u201d to production, from their local shell console. Regular commits, on the other hand, will trigger only the automated tests. Feedback for the entire build process for different configurations are transmitted to a <a href=\"https://slack.com/\">Slack</a> channel\u200a\u2014\u200afor the necessary corrective action, or for splitting traffic between submitted builds in Production once delivery is completed.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*uA8RYnKHkLhJPnzZ\"><figcaption>Fig 5. A representation of the staging CI and stages executed in\u00a0parallel</figcaption></figure><p>Fig 5 above shows the workflow for our staging environment. Cloud Build allows for us to run tasks in parallel, and we are able to retrieve sub-module dependencies as well as install test node modules simultaneously. This is followed by running our test suite once the above requirements are met. Following the successful completion of tests, assets are compiled and uploaded, with setup of the production environment image, and deployment to our staging App Engine application.</p>\n<p><strong>Part II: App Engine Flex, and Stackdriver for monitoring and\u00a0logging</strong></p>\n<p>Next, we\u2019ll lead into the updates for the application to be served via App Engine. When evaluating the options we had, we did not want something like <a href=\"https://cloud.google.com/compute/\">Compute Engine</a>, due to the similarities of using a IaaS as our previous configuration. Kubernetes would have been a better choice due to the higher level of abstraction as compared to Compute Engine and the likes. However, when looking at the offerings in the industry, we found that a PaaS, in this case, App Engine, was much more desirable due to the simpler approach we could take for scaling the application. Our team could concentrate on the front-end features without spending too much time to develop and maintain the infrastructure.</p>\n<p>Within App Engine two choices were presented, <a href=\"https://cloud.google.com/appengine/docs/the-appengine-environments\">Standard Vs Flex</a>. App Engine Flex offers more flexibility in programming language and libraries, where it does not require usage of <a href=\"https://cloud.google.com/appengine/docs/standard/go/issue-requests\">vendor libraries to perform simple tasks</a>, enabling us to prevent vendor locking from occurring. On the other hand, App Engine Standard promises faster scaling response times, deployment times and lower costs. Eventually we choose App Engine Flex, for the flexibility concerns mentioned above. This decision was also made to not repeat the same mistakes of our current WordPress conundrum, which featured heavy coupling of code with plugins that are only effective in their own ecosystem.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*b4zij5k9ZWq-nLiu\"><figcaption>Fig 6. app.yaml example for a App Engine Flex application, with automated scaling\u00a0enabled</figcaption></figure><p>To deploy a new version of your application on App Engine for NodeJS, both `app.yaml` and `package.json` files are required. `app.yaml` will contain the configuration for the machines used and scaling logic for the application, while `package.json` will contain the dependencies, node version and the methodology to start the application.</p>\n<p>From the shell, we can issue commands to deploy using the two files above, and replace the current version that is serving our users. However, as explained in an earlier chapter, we wanted only Cloud Build to have the rights to deploy or deliver the application, for minimizing human error. While deployment for staging do not require any manual intervention, delivery for the production environment require the deploying engineer to verify conditions are right before switching of the live services is done. The changes for production are done using the console as seen\u00a0here:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*mMh0DT0tDbVVlOVB\"><figcaption>Fig 7. App Engine console to control rollout of different versions deployed of an\u00a0app</figcaption></figure><p>For each version, the short hash of the commit is used as its identity. Through identifying each version, we can control the roll out, either by starting, stopping, or letting parallel versions run simultaneously through splitting of the\u00a0traffic.</p>\n<p>With this, we\u2019ve created the CICD pipeline in Cloud Build, and used the App Engine to leverage on the scaling solutions with deployment console to complete the CICD workflow for delivery from staging to production.</p>\n<h3>\n<strong>Building a standard for </strong>performance</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/825/0*GFe6bxIY8ZDvYyGw\"></figure><p>Another issue we wanted to tackle was performance. Performance being the sole responsibility of the server-side is a yesteryear concept. Modern day web browsers are capable of optimizing performance, enabling web services at large to scale optimization efforts for the client-side.</p>\n<p>Yet, the advancement of web technologies are not just seated within the realm of browsers alone. Advancement in methodologies for client-side scripting, has allowed us to be able to tap in newer ways to enhance each user\u2019s experience. One such concept that we have harnessed, lazy-loading, allows us to load content only when the user is viewing it, saving bandwidth and computation resources not only from the server-side, but also with processing it from the client\u2019s\u00a0POV.</p>\n<p>In the following, we\u2019ll share what optimizations were done to improve site performance from the client-side, and advise on the difficulty level.</p>\n<p><strong>Concept #1: Defer load synchronous scripts, async load independent scripts</strong></p>\n<blockquote>\n<strong>Integration Difficulty</strong>: \u2155 (\u2156 if you rely on `$( document ).ready()` from\u00a0jQuery)</blockquote>\n<blockquote>\n<strong>Latency Improvement</strong>: \u2157</blockquote>\n<blockquote>\n<strong>Bandwidth Improvement</strong>: - (Page still loads the same\u00a0content)</blockquote>\n<a href=\"https://medium.com/media/1b5c05afd5b19c37a315920f658eedd8/href\">https://medium.com/media/1b5c05afd5b19c37a315920f658eedd8/href</a><p>As seen in Fig 8 above, incorporating async or defer loading to external scripts involves only a HTML attribute on the \u201cscript\u201d DOM element, which makes it a very easy optimization on top of currently written HTML or view templates. But how do these properties help in the loading of a\u00a0page?</p>\n<p>In Fig 8. above, we see that the both \u201cdefer\u201d and \u201casync\u201d does not block the rendering of the page, allowing the \u201c<a href=\"https://developer.mozilla.org/en-US/docs/Web/Events/DOMContentLoaded\">DOMContentLoaded</a>\u201d event on the document object to trigger much earlier than if there are blocking scripts in the page. What this means for users of our site is that they do not need to be looking at a white screen for extended periods while the page loads, which translates into better perceived latency of the page even if the fully loaded timing remains the same. As reported by KissMetrics in <a href=\"https://blog.kissmetrics.com/wp-content/uploads/2011/04/loading-time.pdf\">2011</a>, \u201cA 1 second delay in page response can result in 7% in conversions\u201d. This is especially relevant to us as a publication site, where we can engage user attention much earlier and lower the bounce\u00a0rate.</p>\n<p>For \u201casync\u201d scripts, these are scripts that should be used if you have independent components, since the load+parse+execute time collectively is not deterministic and does not wait on other content or potential dependencies. \u201cDefer\u201d scripts offer a lax version of \u201casync\u201d, offering to parse and execute scripts in the order that it is declared within the page, regardless of when the scripts are retrieved. This allows us to prevent blocking on page render, yet keeping the dependencies between retrieved script\u00a0assets.</p>\n<p>For existing code bases that are heavily entrenched in popular JavaScript Library jQuery and the usage of $(document).ready()\u00a0, implementing \u201cdefer\u201d on the jQuery script element will be slightly more challenging. This is because $ is undefined at the point of parsing the HTML document.</p>\n<a href=\"https://medium.com/media/2562a32ae557b17670ddb7496fa445d2/href\">https://medium.com/media/2562a32ae557b17670ddb7496fa445d2/href</a><p>Referencing Fig 9 above, this can be changed to use the \u201cDOMContentLoaded\u201d event on the document object, which we have discussed briefly. By using this event, the same initialization behavior from using $(document).ready() can be replicated without having to resort to blocking the DOM render time from loading the jQuery\u00a0library.</p>\n<p><strong>Concept #2: Lazy-loading images, Responsive images, and Progressive image\u00a0loading</strong></p>\n<blockquote>\n<strong>Integration Difficulty</strong>: \u2156</blockquote>\n<blockquote>\n<strong>Latency Improvement</strong>: \u2158</blockquote>\n<blockquote>\n<strong>Bandwidth Improvement</strong>: \u2157</blockquote>\n<a href=\"https://medium.com/media/07d33e7a2fd7f1a05a031a2d9e7fca1c/href\">https://medium.com/media/07d33e7a2fd7f1a05a031a2d9e7fca1c/href</a><p>Lazy-loading of images allows us to delay loading of image assets within the page, until the user has scrolled to the given content. This is especially important to Soompi, where each page is peppered with image assets, given that we want to offer the opportunity for users to explore each page and continue on to other pages. By enabling lazy-loading of images, we can conserve on bandwidth both from server-side as well as the client-side, saving computational costs and resources, as well as better user experience from lower utilization of the clients\u2019 resources (especially that of limited mobile data in today\u2019s\u00a0age).</p>\n<p>Another issue is that responsive web design is done to ensure that the view is optimal for end users with different screen sizes. For image elements, the image sources remains the same for different view ports, resulting in smaller screens loading what can be considered as higher resolution images, and wasting the client\u2019s bandwidth. To optimize this, we can use responsive images as provisioned through the \u201c<a href=\"https://www.w3schools.com/tags/att_source_srcset.asp\">srcset</a>\u201d attribute on img DOM elements.</p>\n<p>By providing a mapping of view port sizes to image URLs for the \u201csrcset\u201d attribute, the browser will ensure that the optimal image to screen resolution will be loaded, deriving the same benefits in saving bandwidth as lazy-loading.</p>\n<p>However, the devil is in the details. Off the bat, integration effort has to be put aside to create a pipeline in the back-end for transforming uploaded images to the different image resolutions. For the front-end, designers and engineers have to experiment to get the appropriate trade-off between image quality and bandwidth, possibly iterating from user feedback as\u00a0well.</p>\n<p>Progressive loading offers a different experience to users as compared to lazy-loading and responsive images\u00a0. Similar to defer loading of scripts above, it improves the perceived latency of the page. This is done by populating the page with lower resolution versions of the images, and then swapping to the intended image once it has been retrieved. By doing this, we\u2019re able to signify that the page is in a state of continuous render towards the final product, instead of showing missing patches in the page where the content is supposed to be and leading to the misinterpretations that there is no progress in the loading of the page or that the page is\u00a0broken.</p>\n<p><strong>Bonus:</strong> One further optimization for images, albeit more reliant on back-end, is to use an image type that is already optimized for loading on web pages\u200a\u2014\u200aand to that, we have webp. In the case of Soompi, we were utilizing a service built in-house in serving webp images. When used in conjunction with a browser that supports webp images, we were able to derive the savings in file sizes for loading the images, further saving bandwidth on both server and client-side.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/613/0*vy9GsA6Hk2m8ycfn\"><figcaption>Fig 11. The image source must return the appropriate response headers in order for the browser to recognize that it can load it as a webp encoded\u00a0image.</figcaption></figure><p>Don\u2019t just take our word for it\u200a\u2014\u200ato quote from a <a href=\"https://developers.google.com/speed/webp/docs/c_study\">study on webp</a>, <em>\u201cWebP typically achieves an average of 30% more compression than JPEG and JPEG 2000, without loss of image\u00a0quality\u201d</em></p>\n<p><strong>Concept #3: Code Splitting</strong></p>\n<blockquote>\n<strong>Integration Difficulty</strong>: \u2158</blockquote>\n<blockquote>\n<strong>Latency Improvement</strong>: \u2158</blockquote>\n<blockquote>\n<strong>Bandwidth Improvement</strong>: \u2158</blockquote>\n<p>Early SPA prototypes presents a dichotomy when loading the web application. It requires all of the existing route components to be loaded upfront. This translates to more resources spent on loading and caching of assets, making the page\u2019s initialization much longer than the rest of the SPA\u2019s life-cycle and even a traditional server-side rendered\u00a0view.</p>\n<p>This situation is not optimal for our publication system, and also likely for many sites out there. Some of our users may only be interested in reading articles that are directly linked from social media platforms, and not interested in traversing the site. Hence, we wanted to employ the concept of code splitting to ensure that users consume the minimal resources they require for using our web application, while maintaining the main benefits of using a\u00a0SPA.</p>\n<p>Implementing code-splitting, on top of SPA with server-side rendering, can be daunting to anyone that are new to these concepts.\u00a0For example, creating logic to instrument the construction of code-splitted components\u00a0in\u00a0the\u00a0module\u00a0bundler\u00a0(such\u00a0as\u00a0Webpack), and then ensuring that these components are appropriately bundled with the correct routes. This alone, is akin to creating something similar to a plugin, to ensure that future developers can implement it intuitively.</p>\n<p>Fortunately\u00a0for\u00a0us, there are public libraries that have already solve this\u00a0problem.</p>\n<a href=\"https://medium.com/media/714855e8daa3a37c31d190520cc8b143/href\">https://medium.com/media/714855e8daa3a37c31d190520cc8b143/href</a><p>As observed in Fig 12 above, we use <a href=\"https://github.com/jamiebuilds/react-loadable\">React-Loadable</a> to create route components that are dynamically imported both the server-side and client-side.</p>\n<p>Each route component only loads when it is needed, and will show a lightweight loading animation if it takes some time to retrieve the route component\u2019s assets.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*OY2DqLmaMEZu9gTHxwKnHw.gif\"><figcaption>Fig 13. For a SPA, traversing in a same session between pages, you can see new code-splitted components being retrieved only when required in a new\u00a0page</figcaption></figure><p><strong>Overall Performance improvements</strong></p>\n<p>A picture speaks more than a thousand words. As such, we have decided to show the metrics that were measured between the application before and after the rewrite, which reveals the results of our efforts over the last year on this\u00a0project.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/766/0*ziweSrz87zT88iuM\"><figcaption>Fig 14. Performance enhancements observed after rewriting the application</figcaption></figure><p>Referencing the improvement concepts\u200a\u2014\u200athe performance improvements above can be divided into three categories: Perceived Latency, Latency and Bandwidth Savings.</p>\n<p>From the above diagram, perceived latency would be related to DOM content loaded time, speed index and start render time (server response), where we saw improvements ranging between 35% to 71% as compared to the older application. Speed index, as defined by Speedcurve, the tool used for the above measurement, \u201c<a href=\"https://speedcurve.com/blog/speedindex-now-available-on-speedcurve/\">is an important measure of a user\u2019s experience as the page loads</a>\u201d. While start render time is not related to our optimizations above, it is a reflection of how improvements of other <a href=\"https://blog.viki.com/modernising-soompi-the-backend-story-b14a493d006e\">related back-end services for WordPress</a> done in parallel to this project, could build into the user experience of the\u00a0page.</p>\n<p>Latency of the page, on the other hand, can be represented by the page loaded time and fully loaded time, which saw an improvement of load times of 56% and 42% respectively.</p>\n<p>For bandwidth savings, it can be represented by the total size of different file content loaded. From these metrics, we observed a 65% bandwidth savings in total file size while providing for the same user experience!</p>\n<h3>Moving Ahead</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hOKywH0gEtLyb1o0.jpg\"></figure><p>So far, we\u2019ve established out new standards for front-end applications through rewriting Soompi\u2019s front-end service.</p>\n<p>Is this the\u00a0end?</p>\n<p>Reflecting on our earlier realization that React was more friendly for reusable components, the same realization was made for the now-denounced CoffeeScript components, in fact just a few years ago! This was due to the fact that RoR was commonly used within the server side at that time. With Ruby being syntactically similar as CoffeeScript, it made translation of related code between the server side Ruby code and client side CoffeeScript easier, reducing context switch for developers and thus making development a much better experience.</p>\n<p>Considering how the trends parallel in the above, its seems that adopting homogeneous solutions for every service to the letter is a dangerous notion. Yet, this does not mean that what we\u2019ve done is contrary to what we\u2019ve achieved. By building a \u201cboilerplate\u201d of solutions that can be used in today\u2019s context, we lay the groundwork for explored principles as a standard, so that the solutions build for today are interchangeable with those arising in the\u00a0future.</p>\n<h3>Other Soompi\u00a0Content</h3>\n<p>Whoa, that was long! Hope you enjoyed the journey we went through so far. The other chapters of our learning are as detailed in the following:</p>\n<a href=\"https://medium.com/media/f051bc15ea51395e40068cba68476447/href\">https://medium.com/media/f051bc15ea51395e40068cba68476447/href</a><h3><strong>Acknowledgement</strong></h3>\n<p>The web team was integral to this rewrite, having both the engineers who supported the legacy application to completing the rewrite, and also in the sharing of this experience. They are as\u00a0follows:</p>\n<p><strong>Aysha</strong> (<a href=\"https://github.com/renettarenula\">github</a>)\u200a\u2014\u200aFront-end warrior princess who led the charge in this\u00a0rewrite</p>\n<p><strong>Amiel</strong> (<a href=\"https://github.com/ikawka\">github</a>)\u200a\u2014\u200aOur WordPress in-house genius, and also ReactJS\u00a0pro</p>\n<p><strong>Weiyuan</strong> (<a href=\"https://gist.github.com/Weiyuan-Lane\">github</a> | <a href=\"http://www.linkedin.com/in/liuweiyuan\">linkedin</a>)\u200a\u2014\u200aSome random guy who also did the\u00a0back-end</p>\n<p>Also, acknowledgement the following engineers / product managers who were part of rewriting this application</p>\n<p><strong>Eric </strong>(<a href=\"https://www.linkedin.com/in/eric-martin-75a1074/\">linkedin</a>)\u200a\u2014\u200aEngineering manager of the web\u00a0team!</p>\n<p><strong>Jonathan</strong> (<a href=\"https://www.linkedin.com/in/armchairtheorist/\">linkedin</a>)\u200a\u2014\u200aProduct manager of awesome-ness.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=14b1483b5c16\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/modernising-soompi-part-3-14b1483b5c16\">Modernizing Soompi\u200a\u2014\u200aPart 3</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3><strong>Modernizing Soompi\u200a\u2014\u200aPart\u00a03</strong></h3>\n<h4><strong>Establishing a new standard for front-end services</strong></h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/794/1*gQhIUB05nCDeBUrhBxFiqA.jpeg\"></figure><p>In an idealistic world, rewriting applications would be the optimal solution for most programming problems. Legacy issues? Rewrite them away! Lost context of what the service entails? Rewrite your understanding in! However, such a naive approach to solving such problems is not always the right way, given real world limitations such as fixed amounts of time to complete projects, and deriving revenue (in turn paying our salaries!) to sustain the project\u2019s momentum.</p>\n<p>One might then ask, \u201cWhy should we embark on this project to rewrite our WordPress service for the front-end\u00a0?\u201d</p>\n<p>The result for us, summarized in one line: \u201cEstablishing a new standard for front-end services\u201d</p>\n<p><strong>Establishing new standards</strong></p>\n<p>\u201cEstablishing a new standard\u201d here suggests a pivotal change in our organisation on how we looked at our front-end applications, and causing substantial reform to invoke ripples of change for the better. At the point of proposing the rewrite for the application, teams at Viki had already tried out different tools. One such tool was ReactJS, which was used in <a href=\"https://www.viki.com/\">Viki\u2019s main site</a>, and to a larger extent, <a href=\"https://www.soompi.com/awards\">Soompi Awards</a>, a branch-off site from Soompi that used React-router to create a Single Page Application (SPA).</p>\n<p>Over time, we found that React was better than other tools we used for front-end components. For example, one benefit we found was that React could be used to create components with very little coupling to the core tech stack. This potential for re-usability was better than our usage of CoffeeScript components, which was coupled to another of our Ruby on Rails (RoR) application.</p>\n<p>The above highlights the notion of building for re-usability, between unrelated services of the immediate team and by extension, the company. By building for more patterns in the same vein as the above, we strive towards building a \u201cboilerplate\u201d of tools that we can use for all front-end services. This benefits the team greatly as context switching is minimized when team members are reassigned between different projects.</p>\n<p>The mentality extended beyond the tech stack\u200a\u2014\u200awe looked at the accumulated performance enhancements that we made in different services, and applied them in this rewrite to solve performance issues we found in our current WordPress application. Infrastructure and scaling was also based on the experiences gained from building micro services using AppEngine, where we observed the benefit in simplifying our team\u2019s core infrastructure responsibilities and skills, allowing us to focus on front-end related feature-sets.</p>\n<h3><strong>Client View as a standard\u200a\u2014\u200aSPA + Isomorphic</strong></h3>\n<p>We started our rewrite with exploring the rendered view for the users. Our WordPress application render content mostly on the server side. This leaves the client side as a \u201cuntapped mine of precious resources\u201d. To that end, we decide to look into the principles of <a href=\"https://developers.google.com/web/fundamentals/architecture/app-shell\">app shell architecture</a>, or the more precise approach, SPA.</p>\n<p>SPA is defined as follows\u200a\u2014\u200aan application that revises part of the view for various combinations of \u201ccontext\u201d. \u201cContext\u201d here can refer to Fully Qualified Domain Name (FQDN), path and state (such as cookies for session usage or GET parameters that denote options for rendering the current\u00a0page).</p>\n<p>For this rewrite, the SPA would translate to implementing logic on the client side that revises the web page when the page\u2019s context changes. This is in stark contrast to our WordPress application, where each HTTP request formed from the same \u201ccontext\u201d result in a newly rendered view from the\u00a0server.</p>\n<p>One might ask: \u201cHow does moving the rendering responsibilities from the server-side to the client-side make for a better user experience?\u201d The answer to that lies in optimizing the behavior of browsers in loading client-facing web pages. Traditionally, loading a page returns a HTML response from the server-side of a service, which in turn provides the instructions for the browser to parse. This is followed by downloading various assets (JavaScript, CSS and font) and creating the DOM elements with the associated event callbacks (not necessary in that\u00a0order).</p>\n<p>For a SPA, loading the first page does the same thing, but subsequent pages do not incur the overhead of reloading and parsing assets as well as the full view. This results in better latency for loading each page due to lesser assets to download, and thus higher levels of user satisfaction, and lower bandwidth costs to our services.</p>\n<p><strong>Implementation</strong></p>\n<p>We used <a href=\"https://github.com/ReactTraining/react-router\">React-Router</a> to create our own SPA. This was done by referencing the current context of the page to different route-centric components. For example, a URL with the path as \u201c/\u201d will utilize the home route component, while a URL with the path of \u201c/article/:id\u201d will utilize the article route component. Changes in the URL through clicking on internal links within the site will trigger an update to the route component currently used.</p>\n<p>It is important to note that the different common functionalities of each page was not coupled to the route components, in order to maximize re-usability and ease of future updates to these functionalities. These functionalities can be divided to two areas\u200a\u2014\u200apage level components and utility level components.</p>\n<a href=\"https://medium.com/media/1eee4b691b830ef1fedfc2611a042f54/href\">https://medium.com/media/1eee4b691b830ef1fedfc2611a042f54/href</a><p>Page level components are as observed in Fig 1 above, through the \u201cNavbar\u201d element. It is safe to assume that the Navbar does not differ between route components for a single web application. Hence, this type of functionalities should be rendered independently from the routes, as seen in \u201crenderRoutes\u201d above.</p>\n<p>Utility level components are the core building blocks of the application. One such example is an \u201cInput\u201d component that gives a warning if the inputted characters exceed the limit passed in through the component\u2019s props. This component can be reused in creating a new user account, or writing a comment to an article. Hence it would be wise to decouple from the route, providing for reusable components within different route components or even page level components.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qKW_7EtLy61CP0gYCm4IcQ.jpeg\"><figcaption>Fig 2. App shell model (Shell component as seen on the left, while on the right it is rendered with\u00a0content)</figcaption></figure><p>In addition to the above, route-centric components must encompass the app shell model. This means that each route component is to be associated to a server-side route, which provides for a response (often JSON) that informs the corresponding component on what should be rendered. There can also be route independent identifiers, such as IDs within the URL (e.g. \u201c/article/1\u201d where \u201c1\u201d is the ID and \u201carticle\u201d references the article component), which are relayed within the request to the server-side for an individualized response. In Fig 2 above, we can see how this is represented, where a route can exist in different states until the content is retrieved.</p>\n<p><strong>SEO and Isopmorphism</strong></p>\n<p>However, a service implementing only a SPA would face problems with Search Engine Optimization (SEO). This is because the initial HTML output of the page only contains the \u201cshell\u201d of the SPA and no contents as observed in Fig 2 above. If any SEO related crawlers does not support the downloading, parsing and execution of JavaScript assets, crawling the page would not culminate to any meaningful search results from just the \u201cshell\u201d\u00a0alone.</p>\n<p>To solve this, we introduce the concept of server-side rendering for the initial load, and inherently creating an Isomorphic Application. A JavaScript engine (e.g. V8) is utilized on the server-side to do the exact same task as the client-side for the initial load. This allows us to preserves the app shell architecture required for subsequent context changes of the page. By virtue of rendering information within the initial load, we allow for bots to retrieve meaningful information that we want to couple with our search\u00a0results.</p>\n<h3><strong>Standardising the infrastructure - Google\u00a0Cloud</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*deKwicY2AmVIi3ty\"></figure><p>Let\u2019s review our infrastructure before we migrated over to <a href=\"https://cloud.google.com/appengine/\">Google App Engine</a>. Our WordPress application was hosted on several manually provisioned VMs (while containerized using Docker) from a cloud provider, gated behind a load balancer. Continuous Integration + Continuous Delivery (CICD) pipeline was done using a variant of Jenkins, with manual deployment using internally developed scripts. As for static assets, such as the compiled JS and CSS, were served from Amazon CloudFront as the Content Delivery Network\u00a0(CDN).</p>\n<p>When analyzing these components of our infrastructure, we observe that the last one was fully furnished by an external partner. Since the CDN component was already providing the benefits that we were looking for in the delivery of static assets, maintained and perfected in its own right, we didn\u2019t sought to work on improving this component. We focused on uplifting the infrastructure for deployment pipeline and application instead.</p>\n<p><strong>Part 1: CICD with Cloud Build, and IAM integrations</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*oDS181_KrUukUT2k\"><figcaption>Fig 3. From IAM menu of Cloud Console\u200a\u2014\u200acontrol permissions of what Cloud Build (service account) can do, such as deploying to App Engine and set 100% of traffic to new version for staging Cloud\u00a0Build.</figcaption></figure><p>For the CICD pipeline updates, we utilized <a href=\"https://cloud.google.com/cloud-build/\">Cloud Build</a>, primarily for a scalable CI experience. This was essential as our in-house CI was hosted on a single machine and would suffer from performance slowdowns or even disruptions when more end-users (developers) were utilizing it.</p>\n<p>Another plus point was that in using Cloud Build, we could leverage benefits for App Engine that was provided by the same partner. One crucial benefit was the usage of <a href=\"https://cloud.google.com/iam/\">Cloud IAM</a>, where we could utilize a privilege scheme to systemize workflow through service accounts used by different Google Cloud tools, as seen in Fig 3. This also meant that by virtue of being in the same ecosystem, there is no need to bake the credentials into our CICD pipeline for the service account that is performing the delivery or deployment.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*zn6-cA6Tf_92JcDC\"><figcaption>Fig 4. Custom role with assigned permissions for any newly on-boarded engineer</figcaption></figure><p>In addition to giving privileges to service accounts, we can also create custom roles for different privilege levels for each engineer. This allows for engineers to have sufficient view rights to monitor the application\u2019s health, but not direct deployment privileges so that checks and balances can be integrated into formal processes (such as enforcing automated tests before automated deployment or delivery) through our CICD pipeline. By employing this arrangement, we can minimize human error by ensuring that only service accounts can perform certain actions, such as deployment.</p>\n<p>In using Cloud Build, we also prepare different configurations for different environments, which are mapped to branch names via regex matching. Using shell scripts that we checked in, and web hook triggers to Cloud Build, developers could either order for a \u201cbuild-and-redeploy\u201d to staging, or \u201cbuild-and-deliver\u201d to production, from their local shell console. Regular commits, on the other hand, will trigger only the automated tests. Feedback for the entire build process for different configurations are transmitted to a <a href=\"https://slack.com/\">Slack</a> channel\u200a\u2014\u200afor the necessary corrective action, or for splitting traffic between submitted builds in Production once delivery is completed.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*uA8RYnKHkLhJPnzZ\"><figcaption>Fig 5. A representation of the staging CI and stages executed in\u00a0parallel</figcaption></figure><p>Fig 5 above shows the workflow for our staging environment. Cloud Build allows for us to run tasks in parallel, and we are able to retrieve sub-module dependencies as well as install test node modules simultaneously. This is followed by running our test suite once the above requirements are met. Following the successful completion of tests, assets are compiled and uploaded, with setup of the production environment image, and deployment to our staging App Engine application.</p>\n<p><strong>Part II: App Engine Flex, and Stackdriver for monitoring and\u00a0logging</strong></p>\n<p>Next, we\u2019ll lead into the updates for the application to be served via App Engine. When evaluating the options we had, we did not want something like <a href=\"https://cloud.google.com/compute/\">Compute Engine</a>, due to the similarities of using a IaaS as our previous configuration. Kubernetes would have been a better choice due to the higher level of abstraction as compared to Compute Engine and the likes. However, when looking at the offerings in the industry, we found that a PaaS, in this case, App Engine, was much more desirable due to the simpler approach we could take for scaling the application. Our team could concentrate on the front-end features without spending too much time to develop and maintain the infrastructure.</p>\n<p>Within App Engine two choices were presented, <a href=\"https://cloud.google.com/appengine/docs/the-appengine-environments\">Standard Vs Flex</a>. App Engine Flex offers more flexibility in programming language and libraries, where it does not require usage of <a href=\"https://cloud.google.com/appengine/docs/standard/go/issue-requests\">vendor libraries to perform simple tasks</a>, enabling us to prevent vendor locking from occurring. On the other hand, App Engine Standard promises faster scaling response times, deployment times and lower costs. Eventually we choose App Engine Flex, for the flexibility concerns mentioned above. This decision was also made to not repeat the same mistakes of our current WordPress conundrum, which featured heavy coupling of code with plugins that are only effective in their own ecosystem.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*b4zij5k9ZWq-nLiu\"><figcaption>Fig 6. app.yaml example for a App Engine Flex application, with automated scaling\u00a0enabled</figcaption></figure><p>To deploy a new version of your application on App Engine for NodeJS, both `app.yaml` and `package.json` files are required. `app.yaml` will contain the configuration for the machines used and scaling logic for the application, while `package.json` will contain the dependencies, node version and the methodology to start the application.</p>\n<p>From the shell, we can issue commands to deploy using the two files above, and replace the current version that is serving our users. However, as explained in an earlier chapter, we wanted only Cloud Build to have the rights to deploy or deliver the application, for minimizing human error. While deployment for staging do not require any manual intervention, delivery for the production environment require the deploying engineer to verify conditions are right before switching of the live services is done. The changes for production are done using the console as seen\u00a0here:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*mMh0DT0tDbVVlOVB\"><figcaption>Fig 7. App Engine console to control rollout of different versions deployed of an\u00a0app</figcaption></figure><p>For each version, the short hash of the commit is used as its identity. Through identifying each version, we can control the roll out, either by starting, stopping, or letting parallel versions run simultaneously through splitting of the\u00a0traffic.</p>\n<p>With this, we\u2019ve created the CICD pipeline in Cloud Build, and used the App Engine to leverage on the scaling solutions with deployment console to complete the CICD workflow for delivery from staging to production.</p>\n<h3>\n<strong>Building a standard for </strong>performance</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/825/0*GFe6bxIY8ZDvYyGw\"></figure><p>Another issue we wanted to tackle was performance. Performance being the sole responsibility of the server-side is a yesteryear concept. Modern day web browsers are capable of optimizing performance, enabling web services at large to scale optimization efforts for the client-side.</p>\n<p>Yet, the advancement of web technologies are not just seated within the realm of browsers alone. Advancement in methodologies for client-side scripting, has allowed us to be able to tap in newer ways to enhance each user\u2019s experience. One such concept that we have harnessed, lazy-loading, allows us to load content only when the user is viewing it, saving bandwidth and computation resources not only from the server-side, but also with processing it from the client\u2019s\u00a0POV.</p>\n<p>In the following, we\u2019ll share what optimizations were done to improve site performance from the client-side, and advise on the difficulty level.</p>\n<p><strong>Concept #1: Defer load synchronous scripts, async load independent scripts</strong></p>\n<blockquote>\n<strong>Integration Difficulty</strong>: \u2155 (\u2156 if you rely on `$( document ).ready()` from\u00a0jQuery)</blockquote>\n<blockquote>\n<strong>Latency Improvement</strong>: \u2157</blockquote>\n<blockquote>\n<strong>Bandwidth Improvement</strong>: - (Page still loads the same\u00a0content)</blockquote>\n<a href=\"https://medium.com/media/1b5c05afd5b19c37a315920f658eedd8/href\">https://medium.com/media/1b5c05afd5b19c37a315920f658eedd8/href</a><p>As seen in Fig 8 above, incorporating async or defer loading to external scripts involves only a HTML attribute on the \u201cscript\u201d DOM element, which makes it a very easy optimization on top of currently written HTML or view templates. But how do these properties help in the loading of a\u00a0page?</p>\n<p>In Fig 8. above, we see that the both \u201cdefer\u201d and \u201casync\u201d does not block the rendering of the page, allowing the \u201c<a href=\"https://developer.mozilla.org/en-US/docs/Web/Events/DOMContentLoaded\">DOMContentLoaded</a>\u201d event on the document object to trigger much earlier than if there are blocking scripts in the page. What this means for users of our site is that they do not need to be looking at a white screen for extended periods while the page loads, which translates into better perceived latency of the page even if the fully loaded timing remains the same. As reported by KissMetrics in <a href=\"https://blog.kissmetrics.com/wp-content/uploads/2011/04/loading-time.pdf\">2011</a>, \u201cA 1 second delay in page response can result in 7% in conversions\u201d. This is especially relevant to us as a publication site, where we can engage user attention much earlier and lower the bounce\u00a0rate.</p>\n<p>For \u201casync\u201d scripts, these are scripts that should be used if you have independent components, since the load+parse+execute time collectively is not deterministic and does not wait on other content or potential dependencies. \u201cDefer\u201d scripts offer a lax version of \u201casync\u201d, offering to parse and execute scripts in the order that it is declared within the page, regardless of when the scripts are retrieved. This allows us to prevent blocking on page render, yet keeping the dependencies between retrieved script\u00a0assets.</p>\n<p>For existing code bases that are heavily entrenched in popular JavaScript Library jQuery and the usage of $(document).ready()\u00a0, implementing \u201cdefer\u201d on the jQuery script element will be slightly more challenging. This is because $ is undefined at the point of parsing the HTML document.</p>\n<a href=\"https://medium.com/media/2562a32ae557b17670ddb7496fa445d2/href\">https://medium.com/media/2562a32ae557b17670ddb7496fa445d2/href</a><p>Referencing Fig 9 above, this can be changed to use the \u201cDOMContentLoaded\u201d event on the document object, which we have discussed briefly. By using this event, the same initialization behavior from using $(document).ready() can be replicated without having to resort to blocking the DOM render time from loading the jQuery\u00a0library.</p>\n<p><strong>Concept #2: Lazy-loading images, Responsive images, and Progressive image\u00a0loading</strong></p>\n<blockquote>\n<strong>Integration Difficulty</strong>: \u2156</blockquote>\n<blockquote>\n<strong>Latency Improvement</strong>: \u2158</blockquote>\n<blockquote>\n<strong>Bandwidth Improvement</strong>: \u2157</blockquote>\n<a href=\"https://medium.com/media/07d33e7a2fd7f1a05a031a2d9e7fca1c/href\">https://medium.com/media/07d33e7a2fd7f1a05a031a2d9e7fca1c/href</a><p>Lazy-loading of images allows us to delay loading of image assets within the page, until the user has scrolled to the given content. This is especially important to Soompi, where each page is peppered with image assets, given that we want to offer the opportunity for users to explore each page and continue on to other pages. By enabling lazy-loading of images, we can conserve on bandwidth both from server-side as well as the client-side, saving computational costs and resources, as well as better user experience from lower utilization of the clients\u2019 resources (especially that of limited mobile data in today\u2019s\u00a0age).</p>\n<p>Another issue is that responsive web design is done to ensure that the view is optimal for end users with different screen sizes. For image elements, the image sources remains the same for different view ports, resulting in smaller screens loading what can be considered as higher resolution images, and wasting the client\u2019s bandwidth. To optimize this, we can use responsive images as provisioned through the \u201c<a href=\"https://www.w3schools.com/tags/att_source_srcset.asp\">srcset</a>\u201d attribute on img DOM elements.</p>\n<p>By providing a mapping of view port sizes to image URLs for the \u201csrcset\u201d attribute, the browser will ensure that the optimal image to screen resolution will be loaded, deriving the same benefits in saving bandwidth as lazy-loading.</p>\n<p>However, the devil is in the details. Off the bat, integration effort has to be put aside to create a pipeline in the back-end for transforming uploaded images to the different image resolutions. For the front-end, designers and engineers have to experiment to get the appropriate trade-off between image quality and bandwidth, possibly iterating from user feedback as\u00a0well.</p>\n<p>Progressive loading offers a different experience to users as compared to lazy-loading and responsive images\u00a0. Similar to defer loading of scripts above, it improves the perceived latency of the page. This is done by populating the page with lower resolution versions of the images, and then swapping to the intended image once it has been retrieved. By doing this, we\u2019re able to signify that the page is in a state of continuous render towards the final product, instead of showing missing patches in the page where the content is supposed to be and leading to the misinterpretations that there is no progress in the loading of the page or that the page is\u00a0broken.</p>\n<p><strong>Bonus:</strong> One further optimization for images, albeit more reliant on back-end, is to use an image type that is already optimized for loading on web pages\u200a\u2014\u200aand to that, we have webp. In the case of Soompi, we were utilizing a service built in-house in serving webp images. When used in conjunction with a browser that supports webp images, we were able to derive the savings in file sizes for loading the images, further saving bandwidth on both server and client-side.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/613/0*vy9GsA6Hk2m8ycfn\"><figcaption>Fig 11. The image source must return the appropriate response headers in order for the browser to recognize that it can load it as a webp encoded\u00a0image.</figcaption></figure><p>Don\u2019t just take our word for it\u200a\u2014\u200ato quote from a <a href=\"https://developers.google.com/speed/webp/docs/c_study\">study on webp</a>, <em>\u201cWebP typically achieves an average of 30% more compression than JPEG and JPEG 2000, without loss of image\u00a0quality\u201d</em></p>\n<p><strong>Concept #3: Code Splitting</strong></p>\n<blockquote>\n<strong>Integration Difficulty</strong>: \u2158</blockquote>\n<blockquote>\n<strong>Latency Improvement</strong>: \u2158</blockquote>\n<blockquote>\n<strong>Bandwidth Improvement</strong>: \u2158</blockquote>\n<p>Early SPA prototypes presents a dichotomy when loading the web application. It requires all of the existing route components to be loaded upfront. This translates to more resources spent on loading and caching of assets, making the page\u2019s initialization much longer than the rest of the SPA\u2019s life-cycle and even a traditional server-side rendered\u00a0view.</p>\n<p>This situation is not optimal for our publication system, and also likely for many sites out there. Some of our users may only be interested in reading articles that are directly linked from social media platforms, and not interested in traversing the site. Hence, we wanted to employ the concept of code splitting to ensure that users consume the minimal resources they require for using our web application, while maintaining the main benefits of using a\u00a0SPA.</p>\n<p>Implementing code-splitting, on top of SPA with server-side rendering, can be daunting to anyone that are new to these concepts.\u00a0For example, creating logic to instrument the construction of code-splitted components\u00a0in\u00a0the\u00a0module\u00a0bundler\u00a0(such\u00a0as\u00a0Webpack), and then ensuring that these components are appropriately bundled with the correct routes. This alone, is akin to creating something similar to a plugin, to ensure that future developers can implement it intuitively.</p>\n<p>Fortunately\u00a0for\u00a0us, there are public libraries that have already solve this\u00a0problem.</p>\n<a href=\"https://medium.com/media/714855e8daa3a37c31d190520cc8b143/href\">https://medium.com/media/714855e8daa3a37c31d190520cc8b143/href</a><p>As observed in Fig 12 above, we use <a href=\"https://github.com/jamiebuilds/react-loadable\">React-Loadable</a> to create route components that are dynamically imported both the server-side and client-side.</p>\n<p>Each route component only loads when it is needed, and will show a lightweight loading animation if it takes some time to retrieve the route component\u2019s assets.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*OY2DqLmaMEZu9gTHxwKnHw.gif\"><figcaption>Fig 13. For a SPA, traversing in a same session between pages, you can see new code-splitted components being retrieved only when required in a new\u00a0page</figcaption></figure><p><strong>Overall Performance improvements</strong></p>\n<p>A picture speaks more than a thousand words. As such, we have decided to show the metrics that were measured between the application before and after the rewrite, which reveals the results of our efforts over the last year on this\u00a0project.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/766/0*ziweSrz87zT88iuM\"><figcaption>Fig 14. Performance enhancements observed after rewriting the application</figcaption></figure><p>Referencing the improvement concepts\u200a\u2014\u200athe performance improvements above can be divided into three categories: Perceived Latency, Latency and Bandwidth Savings.</p>\n<p>From the above diagram, perceived latency would be related to DOM content loaded time, speed index and start render time (server response), where we saw improvements ranging between 35% to 71% as compared to the older application. Speed index, as defined by Speedcurve, the tool used for the above measurement, \u201c<a href=\"https://speedcurve.com/blog/speedindex-now-available-on-speedcurve/\">is an important measure of a user\u2019s experience as the page loads</a>\u201d. While start render time is not related to our optimizations above, it is a reflection of how improvements of other <a href=\"https://blog.viki.com/modernising-soompi-the-backend-story-b14a493d006e\">related back-end services for WordPress</a> done in parallel to this project, could build into the user experience of the\u00a0page.</p>\n<p>Latency of the page, on the other hand, can be represented by the page loaded time and fully loaded time, which saw an improvement of load times of 56% and 42% respectively.</p>\n<p>For bandwidth savings, it can be represented by the total size of different file content loaded. From these metrics, we observed a 65% bandwidth savings in total file size while providing for the same user experience!</p>\n<h3>Moving Ahead</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hOKywH0gEtLyb1o0.jpg\"></figure><p>So far, we\u2019ve established out new standards for front-end applications through rewriting Soompi\u2019s front-end service.</p>\n<p>Is this the\u00a0end?</p>\n<p>Reflecting on our earlier realization that React was more friendly for reusable components, the same realization was made for the now-denounced CoffeeScript components, in fact just a few years ago! This was due to the fact that RoR was commonly used within the server side at that time. With Ruby being syntactically similar as CoffeeScript, it made translation of related code between the server side Ruby code and client side CoffeeScript easier, reducing context switch for developers and thus making development a much better experience.</p>\n<p>Considering how the trends parallel in the above, its seems that adopting homogeneous solutions for every service to the letter is a dangerous notion. Yet, this does not mean that what we\u2019ve done is contrary to what we\u2019ve achieved. By building a \u201cboilerplate\u201d of solutions that can be used in today\u2019s context, we lay the groundwork for explored principles as a standard, so that the solutions build for today are interchangeable with those arising in the\u00a0future.</p>\n<h3>Other Soompi\u00a0Content</h3>\n<p>Whoa, that was long! Hope you enjoyed the journey we went through so far. The other chapters of our learning are as detailed in the following:</p>\n<a href=\"https://medium.com/media/f051bc15ea51395e40068cba68476447/href\">https://medium.com/media/f051bc15ea51395e40068cba68476447/href</a><h3><strong>Acknowledgement</strong></h3>\n<p>The web team was integral to this rewrite, having both the engineers who supported the legacy application to completing the rewrite, and also in the sharing of this experience. They are as\u00a0follows:</p>\n<p><strong>Aysha</strong> (<a href=\"https://github.com/renettarenula\">github</a>)\u200a\u2014\u200aFront-end warrior princess who led the charge in this\u00a0rewrite</p>\n<p><strong>Amiel</strong> (<a href=\"https://github.com/ikawka\">github</a>)\u200a\u2014\u200aOur WordPress in-house genius, and also ReactJS\u00a0pro</p>\n<p><strong>Weiyuan</strong> (<a href=\"https://gist.github.com/Weiyuan-Lane\">github</a> | <a href=\"http://www.linkedin.com/in/liuweiyuan\">linkedin</a>)\u200a\u2014\u200aSome random guy who also did the\u00a0back-end</p>\n<p>Also, acknowledgement the following engineers / product managers who were part of rewriting this application</p>\n<p><strong>Eric </strong>(<a href=\"https://www.linkedin.com/in/eric-martin-75a1074/\">linkedin</a>)\u200a\u2014\u200aEngineering manager of the web\u00a0team!</p>\n<p><strong>Jonathan</strong> (<a href=\"https://www.linkedin.com/in/armchairtheorist/\">linkedin</a>)\u200a\u2014\u200aProduct manager of awesome-ness.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=14b1483b5c16\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/modernising-soompi-part-3-14b1483b5c16\">Modernizing Soompi\u200a\u2014\u200aPart 3</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["wordpress","web","javascript","engineering","google-cloud-platform"]},{"title":"Modernizing Soompi\u200a\u2014\u200aPart 2","pubDate":"2019-04-01 04:52:26","link":"https://blog.viki.com/modernising-soompi-the-backend-story-b14a493d006e?source=rss-669623997d13------2","guid":"https://medium.com/p/b14a493d006e","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/279/1*zTPxht2_KWyWVkrKa5C5FA.png","description":"\n<h3>Modernizing Soompi\u200a\u2014\u200aPart\u00a02</h3>\n<h4>Updating the back-end by deconstructing a\u00a0monolith</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/279/1*zTPxht2_KWyWVkrKa5C5FA.png\"></figure><p>As explained in our problem statement <a href=\"https://blog.viki.com/modernising-soompi-from-a-legacy-435f3782d406\">here</a>, the previous iteration of Soompi had its humble beginnings with running on WordPress. And the notion of using WordPress was noble. Anyone can run it in a bare metal environment / VM, or containerize it effectively to harness its usage within IaaS (Infrastructure as a Service) or PaaS (Platform as a Service) systems. Moreover, \u201ccontinuous development\u201d on WordPress, such as adding new content, encroaches on the level of a SaaS (Software as a Service) instead. This effectively democratizes creation and development of simple applications, breaking down the technical barriers required to sustain new\u00a0content.</p>\n<p>However, running Soompi on top of the WordPress framework over several years has taught us a few things. Firstly, with the many plugins that different administrators of the system have installed over the years, it has effectively became its own monster of a black box. To illustrate this, we once had to migrate the WordPress service between cloud providers. The original average response time ranged between 200ms to 600ms daily. After the migration, the peak grew by 30%! Not only that, the 90th to 99th percentile of the application (which were usually derived from editors who had huge read and write activities from their dashboards) were now recorded on the new system, showing response times between 5s to 10s. This was where editors were starting to realize and complaining about a \u201cvisible\u201d lag in read and write\u00a0actions.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5jekJV1seQCoq6TktrwH3Q.png\"><figcaption>Fig 1. Average response time reported while system migration, which has shown a marked increase and more fluctuations</figcaption></figure><p>After profiling the application, it was found that there was a cache plugin which was contributing to ~90% of the response time. Turning it off helped to reduce response times to an optimum level, but broke some features. This lead us to rollback and restore the plugin, but restoring it retained the improvements in performance, which was baffling to observe first\u00a0hand.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*tFFGjNIZ_amjQbbx.jpg\"><figcaption>Fig 2. Metaphorically spaghetti code, with meatballs as the plugins and the sauce as the mess that is\u00a0caused</figcaption></figure><p>Perhaps that plugin incident was an isolated issue. <em>What\u2019s so bad about plugins anyway</em>, aren\u2019t there similar package managers like NPM that advocate installing and managing plugin-like packages, and that they are black boxes too in their own right, doing what they are expected to do. The counter argument we found to this was that WordPress plugins tend to extend between views, to database operations, and making their own connections (such as API requests), making it less of an isolated feature. This creates a conundrum where discontinuing a plugin that has been installed for a while may break more features than we\u2019re aware of. We can compare this to NPM and its node packages, where the intent of the package is isolated with explicitly defined dependencies\u200a\u2014\u200aallowing us to plug and play node modules more easily than plugins in WordPress, and write regression tests to ensure wrapper features remain working according to its\u00a0specs.</p>\n<p>Another thing we learnt about WordPress was that it was not optimal in handling growing readership. With growing interest in K-pop and readership on our site, and breaking news such as <a href=\"https://www.soompi.com/article/1006957wpp/song-joong-ki-song-hye-kyo-get-married-october\">this</a> occurring, service breakdowns and optimization cycles became more frequent. Eventually, the breaking point was reached where it became increasingly hard to optimize performance without compromising on future maintainability of the system, which sparked the idea to decompose the monolithic application and construct the back-end, our current topic at hand, as its own\u00a0service.</p>\n<blockquote>\n<strong>\u201cWhere do we go from here?\u201d\u200a\u2014\u200a</strong>some developer, probably saying it again\u00a0later</blockquote>\n<h3><strong>\u201cLanguage, the means to communication\"</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/620/0*4lyk638JeWbv6MBz\"><figcaption>Fig 3. What a world we live in today. So much languages to choose\u00a0from!</figcaption></figure><p>As a company, Viki has experimented with NodeJS and Go as back-end services. While there are more developers and interest in JavaScript (such as seen <a href=\"https://octoverse.github.com/projects.html\">here</a>) to fuel development efforts, over time, the company has built up individuals who were able to proficiently develop in Go. This, coupled with the fact that Go was theoretically more performant than NodeJS, by virtue of being compiled over interpreted JavaScript, and able to support concurrency through Go-routines over JavaScript single threaded environment, tips the balance over to Go\u00a0lang.</p>\n<p>In other production services, it has also been recognized internally that Go triumphs in building more robust services for the back-end, making it the go-to for this\u00a0effort.</p>\n<h3><strong>\u201cBuilding a resilient house\u201d</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/935/0*_IYJ_gfcVhlpXY8A.png\"><figcaption>Fig 4. I\u2019ll huff, and I\u2019ll puff, and I\u2019ll blow your house\u00a0down</figcaption></figure><p>Remember the folk tale for Three Little Pigs? With adequate preparation in the form of a brick house, the last of the brothers was able to prevent a disaster from occurring, along with lessons gained from the failed earlier prototypes of straw and stick houses. Similar to that tale, we have to choose our frameworks and libraries wisely, less we become unable to withstand the changes brought about by the breathe of\u00a0time.</p>\n<p>Since we have decided on the language, we move on to the framework/methodology within same ecosystem. As with basic Go web servers, we use the <a href=\"https://golang.org/pkg/net/http/\">net/http</a> native libraries for servicing a HTTP server, and <a href=\"https://github.com/gorilla/mux\">gorilla/mux</a> for the ease of setting up paths, middleware and other benefits that if offers over the default routing libraries.</p>\n<p>With the rising interest in an alternate transport protocol, <a href=\"https://grpc.io/\">gRPC</a>, there was a call to decouple context within the code from the routing logic. Doing this enables us to not only be able to utilize gRPC in the immediate time-frame, but also allows us to plug and play our features to be served from any possible protocols in the future. This was done using <a href=\"https://github.com/go-kit/kit\">Go kit</a>, as observed from the following:</p>\n<a href=\"https://medium.com/media/7b0290880ac3c74ca69cbda40b0e02a0/href\">https://medium.com/media/7b0290880ac3c74ca69cbda40b0e02a0/href</a><p>The same mentality above translated into the \u201csources\u201d for retrieval or insertion of data, where adapters could be created for different types of data stores. These adapters, in turn, provisioned the same interfaces that could be tapped into. This allowed us to provide for the exact same functionality between adapters, to abstract away the differences between the various data\u00a0stores.</p>\n<p>The end results of our effort here were adapters for schema in our chosen data store PostgreSQL, able to be invoked from the endpoints from Go kit as discussed above. Bringing this even further, we created Redis wrappers for caching logic that could be implemented on top of our PostgresSQL schema logic. These Redis wrappers are able to similarly masquerade as the various PostgresSQL schema, allowing it to interface with our PostgresSQL adapter, giving us the freedom to use the cache or direct data store interchangeably since there were no coupling between both of these concerns.</p>\n<p>This culminated to the following structure\u200a\u2014\u200ahaving transports and data stores, both non-application logic components, to be used concurrently or as replacements whenever it is required.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*c0h44meALIgtQIOI0Q9abg.png\"><figcaption>Fig 6. Doesn\u2019t it look like a multi-headed hydra? Hail\u00a0Hydra!</figcaption></figure><h3>\u201cDon\u2019t be\u00a0insane\u201d</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*txgDsHd8cBPJUReh.jpg\"><figcaption>Fig 7. The definition of insanity is doing the same thing over and over and expecting different results\u200a\u2014\u200aAlbert\u00a0Einstein</figcaption></figure><p>Consider this scenario, your team is using a cloud provider. Through their friendly UI, you create a couple of VM instances of differing specifications, along with provisioning a VPC, and maybe some static IPs. Then you develop your applications to deploy on these VMs, and along the way, you start fixing infrastructure bugs that plague your various environments. The project concludes with all the services up and running in production, and your team pat each other on your backs for a job well\u00a0done.</p>\n<p>Or has it\u00a0ended?</p>\n<p>A few months later, your team is tasked with setting up the exact infrastructure for a client of similar requirements. Attempts are made to create the infrastructure, except that the results are always the \u201csame\u201d\u200a\u2014\u200athe infrastructure is always differing from the ideal in one way or another. It could be due to lack of context between developers who hand over the systems from one to another, or even just forgetting the settings that were made from the start. This causes the back-breaking work of initiating the same infrastructure to start all over\u00a0again.</p>\n<p>It begs the question - we check in the code for our applications, why do we not do the same for infrastructure? We could do that to be able to reliably maintain the most updated configurations. Hence enters the premise of Infrastructure as Code\u00a0(IaC).</p>\n<p>To address the identified issue above for this project, a Makefile was checked in as code during the development process, with the desired targets to initialize different permissions. For example, when integrating <a href=\"https://cloud.google.com/cloud-build/\">Cloud Build</a> as the CI and letting it deploy to the staging environment, we need to grant the Cloud Build service account permissions via the IAM console. Within the Makefile, there is also a setup scope to run a template file with config values (can be saved elsewhere) to setup the same infrastructure (VMs, static IPs and so on\u2026) for this project. In our case, we chose a jinja template for use with <a href=\"https://cloud.google.com/deployment-manager/\">Google Deployment Manager</a> since we are using Google Cloud, but there are other applications out there that can do the same, such as <a href=\"https://www.terraform.io/\">Terraform</a>.</p>\n<a href=\"https://medium.com/media/d402e8813c010f31ec4e7aab3e724415/href\">https://medium.com/media/d402e8813c010f31ec4e7aab3e724415/href</a><p>For more information on writing a template for Deployment Manager, see\u00a0<a href=\"https://cloud.google.com/deployment-manager/docs/step-by-step-guide/create-a-template\">here</a>.</p>\n<h3><strong>\u201cKubernetes, captain at the helm with her\u00a0charts\u201d</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/0*32d6ZhN7ZP5IYgb-.png\"><figcaption>Fig 9. Wait?! Is Kubernetes a she or a\u00a0he?</figcaption></figure><p>To get straight to the point, we are using <a href=\"https://kubernetes.io/\">Kubernetes</a>, along with <a href=\"https://helm.sh/\">Helm</a> for usage of its charts for deployment workflows. Our use of Kubernetes also ties in with <a href=\"https://cloud.google.com/kubernetes-engine/\">Kubernetes Engine</a>\u00a0(GKE).</p>\n<p>The move to Kubernetes is an enabler over the previous VM deployment with WordPress. It has allowed us to harness the fast pod scaling capabilities (through declaration of replica count) on top of the auto cluster scaling for instances in GKE. For those unfamiliar with the concept of pods, think of a pod as a single containerized service that you can run on a VM. By changing the config in the deployment YAML file, we can declare the amount of pods whenever there is a need to scale our services.</p>\n<a href=\"https://medium.com/media/5ccf589ab5d92113b8db5e8e54f9e2bb/href\">https://medium.com/media/5ccf589ab5d92113b8db5e8e54f9e2bb/href</a><p>Updates from this config file are submitted by running a single command as\u00a0follows:</p>\n<pre><strong>kubectl apply -f deploymentexample.yaml</strong></pre>\n<p>This was something that we have always desired from our older implementation, where the surge of traffic is unpredictable. By quickly scaling pods and without the time lag in manually scaling instances and setting network config to point to the new pods/containers, we become more operation efficient through minimal service slowdowns or disruptions, and cost efficient by using resources only when we need\u00a0it.</p>\n<p>It should be noted that it is possible to auto-scale pods without having the need to manually update the desired replica count as the above. While this is something we\u2019re investigating as an improvement to our workflow, you can check out <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/\">Horizontal Pod Auto-scaling</a> for auto-scaling your\u00a0pods.</p>\n<p>As for deployment, it was without doubt that the declarative (over imperative) approach for Kubernetes was more suitable, following the last chapter\u2019s approach for checking in the configuration as code. Another point was to \u201ctemplatize\u201d the deployment configuration, for reuse on different environments. We used helm charts to that end. Production and staging configurations are applied only during deployment (through helm install).</p>\n<h3><strong>\u201cNot Really a Soompi\u00a0Anymore\u201d</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/877/1*4Biztkn__OjZtlW-n-QnpQ.jpeg\"><figcaption>Fig 11. A hero can be anyone. Even a man doing something as simple and reassuring as putting a coat around a young boy\u2019s shoulders to let him know the world hadn\u2019t ended \u2014excerpt from The Dark Knight\u00a0Rises</figcaption></figure><p>When his cowl is put on, Bruce Wayne becomes Batman, <em>a living and breathing avatar of justice</em>. However, is the identity of justice exclusive to Batman as Bruce Wayne alone? In fact, anyone could be Batman, even if the contents differs, making another character such as Dick Grayson from a different timeline possible as Batman, despite serving the same\u00a0purpose.</p>\n<p>Publications typically revolve around three things\u200a\u2014\u200aarticles, topics/categories and authors. Using that analogy above and the similarities between publications that we observe, we sought to achieve the base concept of publication as a system\u200a\u2014\u200aa generic publication platform that could be reused in different projects. This inspired our development to decouple the branding of \u201cSoompi\u201d from this system, making it not strictly a component of Soompi, but something that Soompi front-end clients harness to power their readership experiences.</p>\n<p>In fact, the sweet fruits of building from this concept was savored a few months after our first set of services went into production. Last December, we were given the go-ahead to migrate the Spanish variant of <a href=\"https://www.soompi.com/es\">Soompi</a> from WordPress. We were able to do this quickly by spinning up the related set of services, as if it was its own publication service, using the IaC innovations as discussed above. This was a far cry from the difficulties we faced in WordPress, where there were coupled endpoints, database tables and views, causing conflicts against the English publication.</p>\n<h3><strong>Where do we go from\u00a0here?</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/0*bW5_7bUWvr33ijfS.jpg\"><figcaption>Fig 12. There\u2019s still stuff to do, so get back to working,\u00a0Bob!</figcaption></figure><p>The first part of our journey has concluded\u200a\u2014\u200awe\u2019ve achieved what we first set our minds to do, <strong>looking ahead in transport protocols with Gokit and adapters to achieve flexibility in using any data store, easy setup with Deployment Manager, auto-scalable environment against bursts in traffic in Kubernetes, and a homogeneous system capable in supporting any publication client.</strong></p>\n<p>However, the end is\u00a0!(nigh). We discussed about auto-scaling of pods earlier, which is an improvement that can be done to improve our workflow. For our publication service, while it has fulfilled the basic requirements of what a publication needs, plans are in place to further add value to it. For example, user engagement through voting, and gamification of the service, to power clients in their ability to engage their readers and provide better user experience\u200a\u2014\u200a<em>harder, faster, better and stronger</em>.</p>\n<h3>Other Soompi\u00a0Content</h3>\n<p>Our rewrite experience extended beyond the back-end\u200a\u2014\u200acheck out the other parts of our journey\u00a0here:</p>\n<a href=\"https://medium.com/media/f051bc15ea51395e40068cba68476447/href\">https://medium.com/media/f051bc15ea51395e40068cba68476447/href</a><h3><strong>Acknowledgement</strong></h3>\n<p>There are other developers who were also involved in this back-end\u00a0project:</p>\n<p><strong>Omkiran</strong> (<a href=\"https://github.com/omkiran\">github</a> | <a href=\"https://www.linkedin.com/in/omkiran-sharma-999a7632\">linkedin</a>)\u2014 Tribal chief, good direction he\u00a0provides</p>\n<p><strong>Jerome</strong> (<a href=\"https://github.com/jcheng31\">github</a>)\u200a\u2014\u200aSometimes he makes me jealous with the code he\u00a0writes</p>\n<p><strong>Nhat</strong> (<a href=\"https://github.com/Nhat002\">github</a> | <a href=\"https://www.linkedin.com/in/vu-xuan-nhat-tran-19045485/\">linkedin</a> )\u200a\u2014\u200aNhat\u2019s just Nhat, being\u00a0awesome</p>\n<p><strong>Weiyuan</strong> (<a href=\"https://gist.github.com/Weiyuan-Lane\">github</a> | <a href=\"http://www.linkedin.com/in/liuweiyuan\">linkedin</a>)\u200a\u2014\u200aYours\u00a0truly</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b14a493d006e\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/modernising-soompi-the-backend-story-b14a493d006e\">Modernizing Soompi\u200a\u2014\u200aPart 2</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>Modernizing Soompi\u200a\u2014\u200aPart\u00a02</h3>\n<h4>Updating the back-end by deconstructing a\u00a0monolith</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/279/1*zTPxht2_KWyWVkrKa5C5FA.png\"></figure><p>As explained in our problem statement <a href=\"https://blog.viki.com/modernising-soompi-from-a-legacy-435f3782d406\">here</a>, the previous iteration of Soompi had its humble beginnings with running on WordPress. And the notion of using WordPress was noble. Anyone can run it in a bare metal environment / VM, or containerize it effectively to harness its usage within IaaS (Infrastructure as a Service) or PaaS (Platform as a Service) systems. Moreover, \u201ccontinuous development\u201d on WordPress, such as adding new content, encroaches on the level of a SaaS (Software as a Service) instead. This effectively democratizes creation and development of simple applications, breaking down the technical barriers required to sustain new\u00a0content.</p>\n<p>However, running Soompi on top of the WordPress framework over several years has taught us a few things. Firstly, with the many plugins that different administrators of the system have installed over the years, it has effectively became its own monster of a black box. To illustrate this, we once had to migrate the WordPress service between cloud providers. The original average response time ranged between 200ms to 600ms daily. After the migration, the peak grew by 30%! Not only that, the 90th to 99th percentile of the application (which were usually derived from editors who had huge read and write activities from their dashboards) were now recorded on the new system, showing response times between 5s to 10s. This was where editors were starting to realize and complaining about a \u201cvisible\u201d lag in read and write\u00a0actions.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5jekJV1seQCoq6TktrwH3Q.png\"><figcaption>Fig 1. Average response time reported while system migration, which has shown a marked increase and more fluctuations</figcaption></figure><p>After profiling the application, it was found that there was a cache plugin which was contributing to ~90% of the response time. Turning it off helped to reduce response times to an optimum level, but broke some features. This lead us to rollback and restore the plugin, but restoring it retained the improvements in performance, which was baffling to observe first\u00a0hand.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*tFFGjNIZ_amjQbbx.jpg\"><figcaption>Fig 2. Metaphorically spaghetti code, with meatballs as the plugins and the sauce as the mess that is\u00a0caused</figcaption></figure><p>Perhaps that plugin incident was an isolated issue. <em>What\u2019s so bad about plugins anyway</em>, aren\u2019t there similar package managers like NPM that advocate installing and managing plugin-like packages, and that they are black boxes too in their own right, doing what they are expected to do. The counter argument we found to this was that WordPress plugins tend to extend between views, to database operations, and making their own connections (such as API requests), making it less of an isolated feature. This creates a conundrum where discontinuing a plugin that has been installed for a while may break more features than we\u2019re aware of. We can compare this to NPM and its node packages, where the intent of the package is isolated with explicitly defined dependencies\u200a\u2014\u200aallowing us to plug and play node modules more easily than plugins in WordPress, and write regression tests to ensure wrapper features remain working according to its\u00a0specs.</p>\n<p>Another thing we learnt about WordPress was that it was not optimal in handling growing readership. With growing interest in K-pop and readership on our site, and breaking news such as <a href=\"https://www.soompi.com/article/1006957wpp/song-joong-ki-song-hye-kyo-get-married-october\">this</a> occurring, service breakdowns and optimization cycles became more frequent. Eventually, the breaking point was reached where it became increasingly hard to optimize performance without compromising on future maintainability of the system, which sparked the idea to decompose the monolithic application and construct the back-end, our current topic at hand, as its own\u00a0service.</p>\n<blockquote>\n<strong>\u201cWhere do we go from here?\u201d\u200a\u2014\u200a</strong>some developer, probably saying it again\u00a0later</blockquote>\n<h3><strong>\u201cLanguage, the means to communication\"</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/620/0*4lyk638JeWbv6MBz\"><figcaption>Fig 3. What a world we live in today. So much languages to choose\u00a0from!</figcaption></figure><p>As a company, Viki has experimented with NodeJS and Go as back-end services. While there are more developers and interest in JavaScript (such as seen <a href=\"https://octoverse.github.com/projects.html\">here</a>) to fuel development efforts, over time, the company has built up individuals who were able to proficiently develop in Go. This, coupled with the fact that Go was theoretically more performant than NodeJS, by virtue of being compiled over interpreted JavaScript, and able to support concurrency through Go-routines over JavaScript single threaded environment, tips the balance over to Go\u00a0lang.</p>\n<p>In other production services, it has also been recognized internally that Go triumphs in building more robust services for the back-end, making it the go-to for this\u00a0effort.</p>\n<h3><strong>\u201cBuilding a resilient house\u201d</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/935/0*_IYJ_gfcVhlpXY8A.png\"><figcaption>Fig 4. I\u2019ll huff, and I\u2019ll puff, and I\u2019ll blow your house\u00a0down</figcaption></figure><p>Remember the folk tale for Three Little Pigs? With adequate preparation in the form of a brick house, the last of the brothers was able to prevent a disaster from occurring, along with lessons gained from the failed earlier prototypes of straw and stick houses. Similar to that tale, we have to choose our frameworks and libraries wisely, less we become unable to withstand the changes brought about by the breathe of\u00a0time.</p>\n<p>Since we have decided on the language, we move on to the framework/methodology within same ecosystem. As with basic Go web servers, we use the <a href=\"https://golang.org/pkg/net/http/\">net/http</a> native libraries for servicing a HTTP server, and <a href=\"https://github.com/gorilla/mux\">gorilla/mux</a> for the ease of setting up paths, middleware and other benefits that if offers over the default routing libraries.</p>\n<p>With the rising interest in an alternate transport protocol, <a href=\"https://grpc.io/\">gRPC</a>, there was a call to decouple context within the code from the routing logic. Doing this enables us to not only be able to utilize gRPC in the immediate time-frame, but also allows us to plug and play our features to be served from any possible protocols in the future. This was done using <a href=\"https://github.com/go-kit/kit\">Go kit</a>, as observed from the following:</p>\n<a href=\"https://medium.com/media/7b0290880ac3c74ca69cbda40b0e02a0/href\">https://medium.com/media/7b0290880ac3c74ca69cbda40b0e02a0/href</a><p>The same mentality above translated into the \u201csources\u201d for retrieval or insertion of data, where adapters could be created for different types of data stores. These adapters, in turn, provisioned the same interfaces that could be tapped into. This allowed us to provide for the exact same functionality between adapters, to abstract away the differences between the various data\u00a0stores.</p>\n<p>The end results of our effort here were adapters for schema in our chosen data store PostgreSQL, able to be invoked from the endpoints from Go kit as discussed above. Bringing this even further, we created Redis wrappers for caching logic that could be implemented on top of our PostgresSQL schema logic. These Redis wrappers are able to similarly masquerade as the various PostgresSQL schema, allowing it to interface with our PostgresSQL adapter, giving us the freedom to use the cache or direct data store interchangeably since there were no coupling between both of these concerns.</p>\n<p>This culminated to the following structure\u200a\u2014\u200ahaving transports and data stores, both non-application logic components, to be used concurrently or as replacements whenever it is required.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*c0h44meALIgtQIOI0Q9abg.png\"><figcaption>Fig 6. Doesn\u2019t it look like a multi-headed hydra? Hail\u00a0Hydra!</figcaption></figure><h3>\u201cDon\u2019t be\u00a0insane\u201d</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*txgDsHd8cBPJUReh.jpg\"><figcaption>Fig 7. The definition of insanity is doing the same thing over and over and expecting different results\u200a\u2014\u200aAlbert\u00a0Einstein</figcaption></figure><p>Consider this scenario, your team is using a cloud provider. Through their friendly UI, you create a couple of VM instances of differing specifications, along with provisioning a VPC, and maybe some static IPs. Then you develop your applications to deploy on these VMs, and along the way, you start fixing infrastructure bugs that plague your various environments. The project concludes with all the services up and running in production, and your team pat each other on your backs for a job well\u00a0done.</p>\n<p>Or has it\u00a0ended?</p>\n<p>A few months later, your team is tasked with setting up the exact infrastructure for a client of similar requirements. Attempts are made to create the infrastructure, except that the results are always the \u201csame\u201d\u200a\u2014\u200athe infrastructure is always differing from the ideal in one way or another. It could be due to lack of context between developers who hand over the systems from one to another, or even just forgetting the settings that were made from the start. This causes the back-breaking work of initiating the same infrastructure to start all over\u00a0again.</p>\n<p>It begs the question - we check in the code for our applications, why do we not do the same for infrastructure? We could do that to be able to reliably maintain the most updated configurations. Hence enters the premise of Infrastructure as Code\u00a0(IaC).</p>\n<p>To address the identified issue above for this project, a Makefile was checked in as code during the development process, with the desired targets to initialize different permissions. For example, when integrating <a href=\"https://cloud.google.com/cloud-build/\">Cloud Build</a> as the CI and letting it deploy to the staging environment, we need to grant the Cloud Build service account permissions via the IAM console. Within the Makefile, there is also a setup scope to run a template file with config values (can be saved elsewhere) to setup the same infrastructure (VMs, static IPs and so on\u2026) for this project. In our case, we chose a jinja template for use with <a href=\"https://cloud.google.com/deployment-manager/\">Google Deployment Manager</a> since we are using Google Cloud, but there are other applications out there that can do the same, such as <a href=\"https://www.terraform.io/\">Terraform</a>.</p>\n<a href=\"https://medium.com/media/d402e8813c010f31ec4e7aab3e724415/href\">https://medium.com/media/d402e8813c010f31ec4e7aab3e724415/href</a><p>For more information on writing a template for Deployment Manager, see\u00a0<a href=\"https://cloud.google.com/deployment-manager/docs/step-by-step-guide/create-a-template\">here</a>.</p>\n<h3><strong>\u201cKubernetes, captain at the helm with her\u00a0charts\u201d</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/0*32d6ZhN7ZP5IYgb-.png\"><figcaption>Fig 9. Wait?! Is Kubernetes a she or a\u00a0he?</figcaption></figure><p>To get straight to the point, we are using <a href=\"https://kubernetes.io/\">Kubernetes</a>, along with <a href=\"https://helm.sh/\">Helm</a> for usage of its charts for deployment workflows. Our use of Kubernetes also ties in with <a href=\"https://cloud.google.com/kubernetes-engine/\">Kubernetes Engine</a>\u00a0(GKE).</p>\n<p>The move to Kubernetes is an enabler over the previous VM deployment with WordPress. It has allowed us to harness the fast pod scaling capabilities (through declaration of replica count) on top of the auto cluster scaling for instances in GKE. For those unfamiliar with the concept of pods, think of a pod as a single containerized service that you can run on a VM. By changing the config in the deployment YAML file, we can declare the amount of pods whenever there is a need to scale our services.</p>\n<a href=\"https://medium.com/media/5ccf589ab5d92113b8db5e8e54f9e2bb/href\">https://medium.com/media/5ccf589ab5d92113b8db5e8e54f9e2bb/href</a><p>Updates from this config file are submitted by running a single command as\u00a0follows:</p>\n<pre><strong>kubectl apply -f deploymentexample.yaml</strong></pre>\n<p>This was something that we have always desired from our older implementation, where the surge of traffic is unpredictable. By quickly scaling pods and without the time lag in manually scaling instances and setting network config to point to the new pods/containers, we become more operation efficient through minimal service slowdowns or disruptions, and cost efficient by using resources only when we need\u00a0it.</p>\n<p>It should be noted that it is possible to auto-scale pods without having the need to manually update the desired replica count as the above. While this is something we\u2019re investigating as an improvement to our workflow, you can check out <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/\">Horizontal Pod Auto-scaling</a> for auto-scaling your\u00a0pods.</p>\n<p>As for deployment, it was without doubt that the declarative (over imperative) approach for Kubernetes was more suitable, following the last chapter\u2019s approach for checking in the configuration as code. Another point was to \u201ctemplatize\u201d the deployment configuration, for reuse on different environments. We used helm charts to that end. Production and staging configurations are applied only during deployment (through helm install).</p>\n<h3><strong>\u201cNot Really a Soompi\u00a0Anymore\u201d</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/877/1*4Biztkn__OjZtlW-n-QnpQ.jpeg\"><figcaption>Fig 11. A hero can be anyone. Even a man doing something as simple and reassuring as putting a coat around a young boy\u2019s shoulders to let him know the world hadn\u2019t ended \u2014excerpt from The Dark Knight\u00a0Rises</figcaption></figure><p>When his cowl is put on, Bruce Wayne becomes Batman, <em>a living and breathing avatar of justice</em>. However, is the identity of justice exclusive to Batman as Bruce Wayne alone? In fact, anyone could be Batman, even if the contents differs, making another character such as Dick Grayson from a different timeline possible as Batman, despite serving the same\u00a0purpose.</p>\n<p>Publications typically revolve around three things\u200a\u2014\u200aarticles, topics/categories and authors. Using that analogy above and the similarities between publications that we observe, we sought to achieve the base concept of publication as a system\u200a\u2014\u200aa generic publication platform that could be reused in different projects. This inspired our development to decouple the branding of \u201cSoompi\u201d from this system, making it not strictly a component of Soompi, but something that Soompi front-end clients harness to power their readership experiences.</p>\n<p>In fact, the sweet fruits of building from this concept was savored a few months after our first set of services went into production. Last December, we were given the go-ahead to migrate the Spanish variant of <a href=\"https://www.soompi.com/es\">Soompi</a> from WordPress. We were able to do this quickly by spinning up the related set of services, as if it was its own publication service, using the IaC innovations as discussed above. This was a far cry from the difficulties we faced in WordPress, where there were coupled endpoints, database tables and views, causing conflicts against the English publication.</p>\n<h3><strong>Where do we go from\u00a0here?</strong></h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/0*bW5_7bUWvr33ijfS.jpg\"><figcaption>Fig 12. There\u2019s still stuff to do, so get back to working,\u00a0Bob!</figcaption></figure><p>The first part of our journey has concluded\u200a\u2014\u200awe\u2019ve achieved what we first set our minds to do, <strong>looking ahead in transport protocols with Gokit and adapters to achieve flexibility in using any data store, easy setup with Deployment Manager, auto-scalable environment against bursts in traffic in Kubernetes, and a homogeneous system capable in supporting any publication client.</strong></p>\n<p>However, the end is\u00a0!(nigh). We discussed about auto-scaling of pods earlier, which is an improvement that can be done to improve our workflow. For our publication service, while it has fulfilled the basic requirements of what a publication needs, plans are in place to further add value to it. For example, user engagement through voting, and gamification of the service, to power clients in their ability to engage their readers and provide better user experience\u200a\u2014\u200a<em>harder, faster, better and stronger</em>.</p>\n<h3>Other Soompi\u00a0Content</h3>\n<p>Our rewrite experience extended beyond the back-end\u200a\u2014\u200acheck out the other parts of our journey\u00a0here:</p>\n<a href=\"https://medium.com/media/f051bc15ea51395e40068cba68476447/href\">https://medium.com/media/f051bc15ea51395e40068cba68476447/href</a><h3><strong>Acknowledgement</strong></h3>\n<p>There are other developers who were also involved in this back-end\u00a0project:</p>\n<p><strong>Omkiran</strong> (<a href=\"https://github.com/omkiran\">github</a> | <a href=\"https://www.linkedin.com/in/omkiran-sharma-999a7632\">linkedin</a>)\u2014 Tribal chief, good direction he\u00a0provides</p>\n<p><strong>Jerome</strong> (<a href=\"https://github.com/jcheng31\">github</a>)\u200a\u2014\u200aSometimes he makes me jealous with the code he\u00a0writes</p>\n<p><strong>Nhat</strong> (<a href=\"https://github.com/Nhat002\">github</a> | <a href=\"https://www.linkedin.com/in/vu-xuan-nhat-tran-19045485/\">linkedin</a> )\u200a\u2014\u200aNhat\u2019s just Nhat, being\u00a0awesome</p>\n<p><strong>Weiyuan</strong> (<a href=\"https://gist.github.com/Weiyuan-Lane\">github</a> | <a href=\"http://www.linkedin.com/in/liuweiyuan\">linkedin</a>)\u200a\u2014\u200aYours\u00a0truly</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b14a493d006e\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/modernising-soompi-the-backend-story-b14a493d006e\">Modernizing Soompi\u200a\u2014\u200aPart 2</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["cloud-computing","kubernetes","wordpress","docker","engineering"]},{"title":"Modernizing Soompi\u200a\u2014\u200aPart 1","pubDate":"2019-04-01 04:38:11","link":"https://blog.viki.com/modernising-soompi-from-a-legacy-435f3782d406?source=rss-669623997d13------2","guid":"https://medium.com/p/435f3782d406","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*3eqWPoN5ny4hW-VPSu92Xw.png","description":"\n<h3>Modernizing Soompi\u200a\u2014\u200aPart\u00a01</h3>\n<h4>Upgrading a K-pop legend for the new generation</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3eqWPoN5ny4hW-VPSu92Xw.png\"></figure><p>2018 was a monumental year for Soompi; we celebrated our 20th year milestone since serving our first K-pop related news in the late 90s. The journey we took saw us travelling through an era where K-pop and K-dramas became part of mainstream media and no less of a global phenomenon.</p>\n<p>20 years is a long period\u200a\u2014\u200ain the same period, we would have witnessed children growing up to become adults and starting their own lives. As an organisation advancing along in the canals of time, it was within our narrative to redesign our systems, to invoke the same growth as long as it provided wide-reaching benefits for all. In 2018, we embarked on a journey that involved every team of the organisation to carry out this much needed change, and we would like to share our experiences that we have gained along the way\u00a0\u2014</p>\n<a href=\"https://medium.com/media/f051bc15ea51395e40068cba68476447/href\">https://medium.com/media/f051bc15ea51395e40068cba68476447/href</a><h4><strong>Problem, and our statement</strong></h4>\n<p>From a technical perspective, the last 20 years saw the rise in pervasiveness and the benefits of being connected to the world through the Internet, with leaps and bounds gained in the technologies we wield in our everyday lives. 90s\u2019 style websites were scorned as poorly designed for users in the new millennium, and these same expectations iterated again within the next decade towards the 2010s\u00a0. Bulky personal computers shrunk to become mobile devices, capable of connecting folks from all walks of life, at any location, at any time, through a new medium called mobile applications.</p>\n<p>With the wave of technology washing through time, it is important that we sail along and harness useful interpretations of newer technologies to deliver the best experience to our audience. We\u2019ve done this once before, migrating to the WordPress framework as a monolithic application, and harnessing third party vendors in providing the mobile applications.</p>\n<p>The model on WordPress worked for a period of time. It provided for easy integration with back-end components such as databases and caches. Front-end development was relatively simple as compared to building from ground up, allowing editors with an interface to add content easily, and end-users to peruse the content that was provided. There were also ready-made API endpoints that could be integrated with the mobile applications. All these benefits were brought forth with the plugin-friendly WordPress framework and an ecosystem rich in plugins for a wide range of use\u00a0cases.</p>\n<p>Yet it was not meant to be. In recent years, we found that the usage of plugins in WordPress had become our \u201cAchilles heel\u201d. Over-reliance on plugins with its heavy coupling within our application, has lead to stale segments of data and code in the back-end. This has lead to the increase of latency for all services as development ensued over the years. Decomposing the redundant plugins was predicted as a herculean effort, given the magnitude of utilized plugins that rendered the application as a \u201cblack\u00a0box\u201d.</p>\n<p>In some ways, our usage of WordPress was also stretching towards the limits of how the framework should be used. WordPress was originally designed as a publication system for a single writer or a group of writers. Past user engagement campaigns such as Soompi Awards were run on the same system, which led to a buildup in users\u2019 related data. This led to a slowdown in several editor features in WordPress where the underlying database queries were built with less than a hundred rows of user data in mind, now had to content with hundreds of millions of user\u00a0data.</p>\n<p>Another aspect was the front-end. Though the site was not considered too out-dated in terms of the UI design, the technologies used were becoming obsolete given the advancement of client/browser technologies. For example single page applications could be utilized on the front-end to reduce overhead from re-fetching assets and maintaining a local state instead of making additional HTTP requests.</p>\n<p>On the end of mobile applications, a consensus was reached that in-house development was better than sourcing from a third party for custom content. The onus would be on us to perfect the application for our users, as compared to external services where the obligation to improve is limited by contractual terms.</p>\n<p>With the problems and identified above, we decided to embark on rewriting the application\u200a\u2014\u200astarting off with: <a href=\"https://blog.viki.com/modernising-soompi-the-backend-story-b14a493d006e\">Modernizing Soompi\u200a\u2014\u200aObserving from the\u00a0back-end</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=435f3782d406\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/modernising-soompi-from-a-legacy-435f3782d406\">Modernizing Soompi\u200a\u2014\u200aPart 1</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>Modernizing Soompi\u200a\u2014\u200aPart\u00a01</h3>\n<h4>Upgrading a K-pop legend for the new generation</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3eqWPoN5ny4hW-VPSu92Xw.png\"></figure><p>2018 was a monumental year for Soompi; we celebrated our 20th year milestone since serving our first K-pop related news in the late 90s. The journey we took saw us travelling through an era where K-pop and K-dramas became part of mainstream media and no less of a global phenomenon.</p>\n<p>20 years is a long period\u200a\u2014\u200ain the same period, we would have witnessed children growing up to become adults and starting their own lives. As an organisation advancing along in the canals of time, it was within our narrative to redesign our systems, to invoke the same growth as long as it provided wide-reaching benefits for all. In 2018, we embarked on a journey that involved every team of the organisation to carry out this much needed change, and we would like to share our experiences that we have gained along the way\u00a0\u2014</p>\n<a href=\"https://medium.com/media/f051bc15ea51395e40068cba68476447/href\">https://medium.com/media/f051bc15ea51395e40068cba68476447/href</a><h4><strong>Problem, and our statement</strong></h4>\n<p>From a technical perspective, the last 20 years saw the rise in pervasiveness and the benefits of being connected to the world through the Internet, with leaps and bounds gained in the technologies we wield in our everyday lives. 90s\u2019 style websites were scorned as poorly designed for users in the new millennium, and these same expectations iterated again within the next decade towards the 2010s\u00a0. Bulky personal computers shrunk to become mobile devices, capable of connecting folks from all walks of life, at any location, at any time, through a new medium called mobile applications.</p>\n<p>With the wave of technology washing through time, it is important that we sail along and harness useful interpretations of newer technologies to deliver the best experience to our audience. We\u2019ve done this once before, migrating to the WordPress framework as a monolithic application, and harnessing third party vendors in providing the mobile applications.</p>\n<p>The model on WordPress worked for a period of time. It provided for easy integration with back-end components such as databases and caches. Front-end development was relatively simple as compared to building from ground up, allowing editors with an interface to add content easily, and end-users to peruse the content that was provided. There were also ready-made API endpoints that could be integrated with the mobile applications. All these benefits were brought forth with the plugin-friendly WordPress framework and an ecosystem rich in plugins for a wide range of use\u00a0cases.</p>\n<p>Yet it was not meant to be. In recent years, we found that the usage of plugins in WordPress had become our \u201cAchilles heel\u201d. Over-reliance on plugins with its heavy coupling within our application, has lead to stale segments of data and code in the back-end. This has lead to the increase of latency for all services as development ensued over the years. Decomposing the redundant plugins was predicted as a herculean effort, given the magnitude of utilized plugins that rendered the application as a \u201cblack\u00a0box\u201d.</p>\n<p>In some ways, our usage of WordPress was also stretching towards the limits of how the framework should be used. WordPress was originally designed as a publication system for a single writer or a group of writers. Past user engagement campaigns such as Soompi Awards were run on the same system, which led to a buildup in users\u2019 related data. This led to a slowdown in several editor features in WordPress where the underlying database queries were built with less than a hundred rows of user data in mind, now had to content with hundreds of millions of user\u00a0data.</p>\n<p>Another aspect was the front-end. Though the site was not considered too out-dated in terms of the UI design, the technologies used were becoming obsolete given the advancement of client/browser technologies. For example single page applications could be utilized on the front-end to reduce overhead from re-fetching assets and maintaining a local state instead of making additional HTTP requests.</p>\n<p>On the end of mobile applications, a consensus was reached that in-house development was better than sourcing from a third party for custom content. The onus would be on us to perfect the application for our users, as compared to external services where the obligation to improve is limited by contractual terms.</p>\n<p>With the problems and identified above, we decided to embark on rewriting the application\u200a\u2014\u200astarting off with: <a href=\"https://blog.viki.com/modernising-soompi-the-backend-story-b14a493d006e\">Modernizing Soompi\u200a\u2014\u200aObserving from the\u00a0back-end</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=435f3782d406\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/modernising-soompi-from-a-legacy-435f3782d406\">Modernizing Soompi\u200a\u2014\u200aPart 1</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["web-development","wordpress","web","engineering","website"]},{"title":"How we use web browser extensions to build incremental web features at Viki","pubDate":"2019-01-22 03:29:03","link":"https://blog.viki.com/more-than-a-shower-thought-chrome-browser-extensions-instead-of-frontend-web-services-6d1c4d81a136?source=rss-669623997d13------2","guid":"https://medium.com/p/6d1c4d81a136","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*IVqyAvStmBd1y2BuO0jccQ.png","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IVqyAvStmBd1y2BuO0jccQ.png\"></figure><p>Recently, we had a problem that we needed to solve. We needed to add a new feature to our <a href=\"https://www.soompi.com/\">Soompi</a> website that gave our editorial staff \u201cone-click access\u201d to edit an article. However, that feature was only to be limited to a small set of users (our editorial staff), and will not be used by anyone\u00a0else.</p>\n<p>It\u2019s relatively simple to achieve that in a monolithic service where staff related services are housed on the same application as the web client. But modern day applications don\u2019t tend to work that way\u200a\u2014\u200awe advocate developing decoupled yet highly cohesive services instead, or what is known as <a href=\"https://web.archive.org/web/20160819141303/http://opengroup.org/soa/source-book/soa/soa.htm\">Service-Oriented Architecture</a> (SOA).</p>\n<p>With decoupled services comes with teams that are usually in charge of different services\u200a\u2014\u200awhich makes cross development difficult. Just to mention a few issues faced\u200a\u2014\u200asuch as different languages or frameworks utilized. Or time to familiarize with the code base before doing anything ambitious. Perhaps even small differences in team culture makes it even harder to quickly acclimate and facilitate agile development.</p>\n<p>In light of this problem, why not write a browser extension instead of creating or augmenting services as the solution. One might even argue that this is an independent service, where the inputs are the URL and the rendered HTML within the page, while the outputs are the desired hidden content, redirects and toggles within the page, reserved for the intended audience where the extension is made available to.</p>\n<p><strong>So, where do we\u00a0start?</strong></p>\n<ol>\n<li>\n<strong>Decide the browser\u200a\u2014</strong>\u200aEveryone may not have the same preferred browser, which makes adoption not 100% foolproof. After internal discussions and checking browser usage, Chrome seem like the <a href=\"https://caniuse.com/usage-table\">clear choice</a>. Its popularity ahead of other browsers makes it more easily marketable to my colleagues.</li>\n<li>\n<strong>Decide the feature\u200a\u2014</strong>\u200aFeatures can be implemented from the <a href=\"https://developer.chrome.com/extensions/contextMenus\">context menu</a> (AKA right click menu) - which services basic redirects and view manipulation. Another element of the Chrome Extension API is the browser action (the icon on the top right hand of the browser) and <a href=\"https://developer.chrome.com/extensions/browserAction#popups\">popup</a> - a separate view rendered on top of your web page, giving you more liberty in designing elements to interact with other services. One thing to note here is that this is not server-side logic, and I would recommend against adding logic that is prone to creating security risks for yourself or your\u00a0company.</li>\n<li>\n<strong>(EXTRA) Decide the framework</strong> (and to a lesser extent, libraries too)<strong>\u200a\u2014\u200a</strong>If you\u2019re implementing a popup, why not write it on a JS framework like Angular or library like React. The benefit that comes with the above are the ease of integrating tools that are already publicly available (such as those that can be installed using NPM). Most importantly, these items have accompanying standards and modularized ways to structure the code, something that is a must-have for ensuring that future developers can understand and build on top of, with\u00a0ease.</li>\n<li>\n<strong>Publication cost\u200a\u2014</strong>\u200aLastly, prepare <a href=\"https://developer.chrome.com/webstore/publish\">$5</a> before you publish your application. You have to spend this dough here, but the silver lining is that this is a one time cost for publishing 20 applications under your\u00a0account.</li>\n</ol>\n<p><strong>Implementation</strong></p>\n<p>For this extension, we will be creating a simple contextual menu that adds options depending on the page we\u2019re on. We\u2019ll also change the color of the browser action icon when its on our domain\u200a\u2014\u200aadditional UX design here to hint that the extension is available when visiting our\u00a0website.</p>\n<p>Before you start, you need to prepare a manifest file, for the various entry points of scripts and views, image assets, and information about your extension.</p>\n<a href=\"https://medium.com/media/f36b567365c38ac08db73dcd73efc5f0/href\">https://medium.com/media/f36b567365c38ac08db73dcd73efc5f0/href</a><p>From the above, note the items that are crucial to the feature sets of this extension: context-menu, browser-action and\u00a0popup.</p>\n<p>For context-menu, it can be done very quickly by providing the name of the menu items and callbacks destined to be in the right-click menu in a single script. However, this could also make it hard to maintain\u200a\u2014\u200aso why not modularize it? To start, we need a building block for a menu\u00a0item.</p>\n<a href=\"https://medium.com/media/a38f5560bc5ddb1cadbd35cb7d42dfd1/href\">https://medium.com/media/a38f5560bc5ddb1cadbd35cb7d42dfd1/href</a><p>From the above we established a base class that can used to modularize our context menu logic. Here\u2019s how it was done for the root menu item that was implemented with MenuItem.</p>\n<a href=\"https://medium.com/media/da7ec29c43084f54c5380fce3116ed5c/href\">https://medium.com/media/da7ec29c43084f54c5380fce3116ed5c/href</a><p>While we did not establish a callback here, you could implement it in any of your other menu item instances, and perhaps trigger a different URL in a different tab:</p>\n<pre>chrome.tabs.create({ url: URL })</pre>\n<p>And finally, context-menu.js to initialize the menu items (in a Depth-first search like\u00a0manner)</p>\n<a href=\"https://medium.com/media/e5957a5fe1897c83af46ae7c4f901ad9/href\">https://medium.com/media/e5957a5fe1897c83af46ae7c4f901ad9/href</a><p>Next off, we have browser-action which deals with the logic of how the icon is displayed in Chrome\u00a0toolbar</p>\n<a href=\"https://medium.com/media/f736a38d4a97d97b73079bac140414e5/href\">https://medium.com/media/f736a38d4a97d97b73079bac140414e5/href</a><p>The implementation is relatively simple\u200a\u2014\u200awe establish a means to update the icon as the faded variant and the regular variant. The mechanism is triggered by two listened events\u200a\u2014\u200awhen the URL is updated for the currently active tab, and when the active tab in Chrome is\u00a0changed.</p>\n<p>The above was done in the context of <a href=\"https://www.soompi.com/\">Soompi</a>, where we had identified with the problem statement earlier. Another innovation that was not discussed here was the addition of React modules in the rendered popup.html, triggered by clicking the browser action icon. This is similar to web services that uses React, with a rendered HTML view that includes the scripts which invokes the relevant components for various utilities.</p>\n<p>In retrospect, this isn\u2019t a novel idea. In fact, someone I knew already worked on a similar extension tool before, which gave more weight to making this a reality even as a side project. If anything, it drove us to create a modern Chrome extension with newer offerings backed with relevant technologies used in our production web services, instead of defaulting to vanilla JavaScript.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d1c4d81a136\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/more-than-a-shower-thought-chrome-browser-extensions-instead-of-frontend-web-services-6d1c4d81a136\">How we use web browser extensions to build incremental web features at Viki</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IVqyAvStmBd1y2BuO0jccQ.png\"></figure><p>Recently, we had a problem that we needed to solve. We needed to add a new feature to our <a href=\"https://www.soompi.com/\">Soompi</a> website that gave our editorial staff \u201cone-click access\u201d to edit an article. However, that feature was only to be limited to a small set of users (our editorial staff), and will not be used by anyone\u00a0else.</p>\n<p>It\u2019s relatively simple to achieve that in a monolithic service where staff related services are housed on the same application as the web client. But modern day applications don\u2019t tend to work that way\u200a\u2014\u200awe advocate developing decoupled yet highly cohesive services instead, or what is known as <a href=\"https://web.archive.org/web/20160819141303/http://opengroup.org/soa/source-book/soa/soa.htm\">Service-Oriented Architecture</a> (SOA).</p>\n<p>With decoupled services comes with teams that are usually in charge of different services\u200a\u2014\u200awhich makes cross development difficult. Just to mention a few issues faced\u200a\u2014\u200asuch as different languages or frameworks utilized. Or time to familiarize with the code base before doing anything ambitious. Perhaps even small differences in team culture makes it even harder to quickly acclimate and facilitate agile development.</p>\n<p>In light of this problem, why not write a browser extension instead of creating or augmenting services as the solution. One might even argue that this is an independent service, where the inputs are the URL and the rendered HTML within the page, while the outputs are the desired hidden content, redirects and toggles within the page, reserved for the intended audience where the extension is made available to.</p>\n<p><strong>So, where do we\u00a0start?</strong></p>\n<ol>\n<li>\n<strong>Decide the browser\u200a\u2014</strong>\u200aEveryone may not have the same preferred browser, which makes adoption not 100% foolproof. After internal discussions and checking browser usage, Chrome seem like the <a href=\"https://caniuse.com/usage-table\">clear choice</a>. Its popularity ahead of other browsers makes it more easily marketable to my colleagues.</li>\n<li>\n<strong>Decide the feature\u200a\u2014</strong>\u200aFeatures can be implemented from the <a href=\"https://developer.chrome.com/extensions/contextMenus\">context menu</a> (AKA right click menu) - which services basic redirects and view manipulation. Another element of the Chrome Extension API is the browser action (the icon on the top right hand of the browser) and <a href=\"https://developer.chrome.com/extensions/browserAction#popups\">popup</a> - a separate view rendered on top of your web page, giving you more liberty in designing elements to interact with other services. One thing to note here is that this is not server-side logic, and I would recommend against adding logic that is prone to creating security risks for yourself or your\u00a0company.</li>\n<li>\n<strong>(EXTRA) Decide the framework</strong> (and to a lesser extent, libraries too)<strong>\u200a\u2014\u200a</strong>If you\u2019re implementing a popup, why not write it on a JS framework like Angular or library like React. The benefit that comes with the above are the ease of integrating tools that are already publicly available (such as those that can be installed using NPM). Most importantly, these items have accompanying standards and modularized ways to structure the code, something that is a must-have for ensuring that future developers can understand and build on top of, with\u00a0ease.</li>\n<li>\n<strong>Publication cost\u200a\u2014</strong>\u200aLastly, prepare <a href=\"https://developer.chrome.com/webstore/publish\">$5</a> before you publish your application. You have to spend this dough here, but the silver lining is that this is a one time cost for publishing 20 applications under your\u00a0account.</li>\n</ol>\n<p><strong>Implementation</strong></p>\n<p>For this extension, we will be creating a simple contextual menu that adds options depending on the page we\u2019re on. We\u2019ll also change the color of the browser action icon when its on our domain\u200a\u2014\u200aadditional UX design here to hint that the extension is available when visiting our\u00a0website.</p>\n<p>Before you start, you need to prepare a manifest file, for the various entry points of scripts and views, image assets, and information about your extension.</p>\n<a href=\"https://medium.com/media/f36b567365c38ac08db73dcd73efc5f0/href\">https://medium.com/media/f36b567365c38ac08db73dcd73efc5f0/href</a><p>From the above, note the items that are crucial to the feature sets of this extension: context-menu, browser-action and\u00a0popup.</p>\n<p>For context-menu, it can be done very quickly by providing the name of the menu items and callbacks destined to be in the right-click menu in a single script. However, this could also make it hard to maintain\u200a\u2014\u200aso why not modularize it? To start, we need a building block for a menu\u00a0item.</p>\n<a href=\"https://medium.com/media/a38f5560bc5ddb1cadbd35cb7d42dfd1/href\">https://medium.com/media/a38f5560bc5ddb1cadbd35cb7d42dfd1/href</a><p>From the above we established a base class that can used to modularize our context menu logic. Here\u2019s how it was done for the root menu item that was implemented with MenuItem.</p>\n<a href=\"https://medium.com/media/da7ec29c43084f54c5380fce3116ed5c/href\">https://medium.com/media/da7ec29c43084f54c5380fce3116ed5c/href</a><p>While we did not establish a callback here, you could implement it in any of your other menu item instances, and perhaps trigger a different URL in a different tab:</p>\n<pre>chrome.tabs.create({ url: URL })</pre>\n<p>And finally, context-menu.js to initialize the menu items (in a Depth-first search like\u00a0manner)</p>\n<a href=\"https://medium.com/media/e5957a5fe1897c83af46ae7c4f901ad9/href\">https://medium.com/media/e5957a5fe1897c83af46ae7c4f901ad9/href</a><p>Next off, we have browser-action which deals with the logic of how the icon is displayed in Chrome\u00a0toolbar</p>\n<a href=\"https://medium.com/media/f736a38d4a97d97b73079bac140414e5/href\">https://medium.com/media/f736a38d4a97d97b73079bac140414e5/href</a><p>The implementation is relatively simple\u200a\u2014\u200awe establish a means to update the icon as the faded variant and the regular variant. The mechanism is triggered by two listened events\u200a\u2014\u200awhen the URL is updated for the currently active tab, and when the active tab in Chrome is\u00a0changed.</p>\n<p>The above was done in the context of <a href=\"https://www.soompi.com/\">Soompi</a>, where we had identified with the problem statement earlier. Another innovation that was not discussed here was the addition of React modules in the rendered popup.html, triggered by clicking the browser action icon. This is similar to web services that uses React, with a rendered HTML view that includes the scripts which invokes the relevant components for various utilities.</p>\n<p>In retrospect, this isn\u2019t a novel idea. In fact, someone I knew already worked on a similar extension tool before, which gave more weight to making this a reality even as a side project. If anything, it drove us to create a modern Chrome extension with newer offerings backed with relevant technologies used in our production web services, instead of defaulting to vanilla JavaScript.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d1c4d81a136\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/more-than-a-shower-thought-chrome-browser-extensions-instead-of-frontend-web-services-6d1c4d81a136\">How we use web browser extensions to build incremental web features at Viki</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["javascript","wordpress","engineering","chrome-extension","web-development"]},{"title":"How to supercharge Continuous Integration (CI) tools for your workplace with managed CI!","pubDate":"2018-12-21 03:10:13","link":"https://blog.viki.com/how-to-supercharge-continuous-integration-ci-tools-for-your-workplace-with-managed-ci-f97c8c09c63f?source=rss-669623997d13------2","guid":"https://medium.com/p/f97c8c09c63f","author":"Weiyuan Liu","thumbnail":"https://cdn-images-1.medium.com/max/816/0*pG5zK5AZGLTeVkHw","description":"\n<h3>How we supercharge Continuous Integration (CI) tools at Rakuten\u00a0Viki!</h3>\n<p>Some time ago, my colleague and manager of our platform team at <a href=\"http://www.viki.com/\">Rakuten Viki</a>, Omkiran, started a company-wide conversation on re-hauling most of our entire infrastructure. This was done at a time where several of our services were deployed within Virtual Machines (VMs), essentially using Infrastructure as a Service (IaaS) services. The premise of the conversation was as follows\u00a0\u2014</p>\n<blockquote>If limited resources of an organisation, such as engineers, were to be tied down with solving low level tasks like managing the health and operability of various processes, how can we, as a company, spare resources as we scale up in services? How can we be able to automate our entire infrastructure, so that we can move on and innovate on newer technologies, such as Machine Learning?</blockquote>\n<p>The proposed solution was to make use of cloud providers, or more specifically to today\u2019s implementation, <strong>Google Cloud Platform\u00a0(GCP)</strong>.</p>\n<p>First, let us discuss why cloud providers are essential to meeting the highlighted premise\u00a0above:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/816/0*pG5zK5AZGLTeVkHw\"><figcaption>Fig 1. Comparing cloud service management requirements with physical solutions (<a href=\"https://jelastic.com/blog/growing-paas-market-benefits-for-enterprises-service-providers-and-developers-dailyhostnews/\">original article</a>)</figcaption></figure><ul>\n<li>Localized server racks suffer from the requirement to provide continuous support for physical assets. Hardware and software changes are challenges faced at this\u00a0level.</li>\n<li>VMs provided by IaaS providers assuage the hardware requirements in Fig 1. However, we\u2019ll still face software challenges like managing OS upgrades, updates and scaling to\u00a0demand.</li>\n</ul>\n<p>In the above, we see that both hardware and software pose challenges, to continue operations and ever-expanding business requirements of a growing organization. These challenges, at large, have already been solved by the industry as seen in Fig\u00a01.</p>\n<p><strong>This leads us to turn to cloud providers, which provide managed services, such as Platform as a Service (PaaS, so that we can free engineering resources and focus on solving challenges that are specific to growing our organization.</strong></p>\n<p><strong>But how does this change tie in with CI\u00a0tooling?</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/691/1*3_0oyZzlzLOrlZZKLGsfoA.png\"><figcaption>Fig 2. Why? Why?\u00a0Why?</figcaption></figure><p><strong>Firstly, what does CI mean to Viki:</strong><br>Services in Viki uses CI in the development part of our pipeline, often for automating the execution of new and regression tests to validate changes under multi-branch development. There are other usages pertaining to specific teams, such as pre-compiling assets for front-end services, and creating APK files for Android applications. Sometimes it goes beyond CI, using a limited form of Continuous Deployment (CD) for staging and canary environments in the delivery mindset of the\u00a0service.</p>\n<p><strong>How was CI done in Viki?</strong><br>Before, we used three different CI tools\u200a\u2014\u200atwo different versions of Drone, and Jenkins, in carrying out CI within the company. The CI tools were deployed as internal services, on top of provisioned VMs, similar to external services.</p>\n<p><strong>Why should CI change to be a managed tool, if it is an internal service and not tied to client (user) utilization?<br></strong>Client linked services have an explicit requirement to auto-scale as the users increase or decrease, which is already provided for in most PaaS services. In the case of CI, it is the \u201cservice\u201d for our services as its \u201cusers\u201d. As the number of services developed by the organization increases, the same issues faced by client services surface\u200a\u2014\u200anamely increased latency along with more frequent failures. We can vertically or horizontally scale the machines which power our CI to solve this issue, but reserving better or more instances does not make for cost efficiency where the engineering core is based in similar time zones (usage is low at night) and CI jobs differ in required processing power resulting in unpredictable fluctuations in usage. Horizontal auto-scaling is the solution here, but it would have to either be already supported by the CI, or we have to build custom logic to spin up and spin down VMs whenever it is required.</p>\n<p>Another issue was supporting multiple versions of CI or different CI tools altogether. Unfortunately, this was a result of incomplete migrations and incompatible instructions across versions - which leads to another familiar conundrum we have seen in Fig 1\u200a\u2014\u200amaintenance of different \u201cOS\u201d.</p>\n<p><strong>How does GCP solve these problems?</strong></p>\n<p>We looked at GCP\u2019s offering, <a href=\"https://cloud.google.com/cloud-build/\">Cloud Build</a>, for the problems we faced with present iterations of our\u00a0CI.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/proxy/1*p6Nc3zVTadBSQDd7cXvzDw.png\"><figcaption>Fig 3. Cloud\u00a0Build!</figcaption></figure><p>Firstly, I have to declare that Cloud Build by itself is not a complete CI tool. Instead, I would categorize it as the base which anyone can build kick-ass CI tools\u00a0on.</p>\n<p>Cloud Build works by providing a trigger configuration and build configuration. The trigger configuration is as\u00a0follows:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/615/1*s0EuORFo8X0rkndrsN992g.png\"><figcaption>Fig 4. Cloud Build trigger configuration</figcaption></figure><p>By linking to either Github, Bitbucket, or Google Code Repository, builds can be configured to trigger on watched files, or everything-but-ignored files, on top of selected branches (with regex to provide selection criteria, but mind you, no negative look-ahead supported at this point). Another option is providing a Dockerfile or a configuration file (YAML) specifically for Cloud Build, as the build configuration required to pass or fail a triggered build.</p>\n<p>For Viki, we opted to use the Cloud Build\u2019s configuration file as a common standard, as opposed to a Dockerfile, due to the fact that the \u201cbase images\u201d of the Cloud Build\u2019s configuration are essentially machine types (of differing processing power and number of cores). This solves our problem of latency and failures due to scaling issues, and even allow us to speed up individual builds for critical services as and when we\u00a0need.</p>\n<p>The following configuration shows a Node application with test and linter checks, along with building an image for deployment:</p>\n<pre>steps:<br>- id: retrieveNodeModules<br>  waitFor: ['-']<br>  name: gcr.io/${PROJECT_ID}/retrieve:1.0.0<br>  args: [$REPO_NAME, 'nodeModules']<br><br>- id: installTestNpm<br>  waitFor: ['retrieveNodeModules']<br>  name: 'gcr.io/cloud-builders/npm'<br>  args: ['--loglevel=error', 'install', '--prefer-offline']</pre>\n<pre>- id: runJasmineTests<br>  waitFor: ['installTestNpm']<br>  name: 'gcr.io/cloud-builders/npm'<br>  args: ['run', 'jasmine']<br><br>- id: runLinterChecks<br>  waitFor: ['installTestNpm']<br>  name: 'gcr.io/cloud-builders/npm'<br>  args: ['run', 'linter']<br><br>- id: cacheNodeModules<br>  waitFor: ['installTestNpm']<br>  name: gcr.io/${PROJECT_ID}/cache:1.0.0<br>  args: [$REPO_NAME, 'nodeModules']</pre>\n<pre>- id: dockerBuild<br>  waitFor: ['runJasmineTests', 'runLinterChecks']<br>  name: 'gcr.io/cloud-builders/docker'<br>  args: ['build', '-f', 'Dockerfile', '-t', 'gcr.io/$PROJECT_ID/project:$SHORT_SHA', '.']<br><br>- id: dockerPush<br>  waitFor: ['dockerBuild']<br>  name: 'gcr.io/cloud-builders/docker'<br>  args: ['push', 'gcr.io/$PROJECT_ID/project:$SHORT_SHA', '.']</pre>\n<pre>timeout: 1800s<br><br>options:<br>  machineType: 'N1_HIGHCPU_8'</pre>\n<p>From the above example, we see a few configuration properties. The first thing we\u2019ll touch on would be the steps\u2019 properties for the build. \u201cname\u201d and \u201cargs\u201d are synonymous to the common usage of Docker images, in identifying the image used and arguments in spinning up the image as a dockerized container. \u201cid\u201d and \u201cwaitFor\u201d creates the build dependency between steps, allowing for steps to execute in parallel as discussed before.</p>\n<p>Another property here is \u201cmachineType\u201d, where one can select the machine to run the build on. This stacks with the ability to run builds in parallel, whereby selecting a multi-core machine will help to speed up build\u00a0time.</p>\n<p>For some CI tools, a single build run steps serially in a unique environment. This is a result of each service with their own requirements. By using Cloud Build\u2019s steps as seen above, this translates to the advantage that we can create helper steps which can be shared across different services\u200a\u2014\u200asuch as caching utility to speed up build times, or deployment steps to Kubernetes using helm to staging or canary clusters.</p>\n<p>Usage of Cloud Build expands even further with the use of GCP. In the above configuration YAML file, the Docker images of the different steps come from both public and private image repositories hosted on GCP. Through the use of service accounts, we can enable Cloud Build to access private images, without having to configure and manage complex access management, such as creating and uploading public-private SSH key-pairs.</p>\n<p>Better still, only the administrators could perform access management rights\u2019 assignments, and regular developers possess only read privileges. This safeguards our CI from failures caused by accidental changes.</p>\n<p>From the above, we have created an automated build pipeline for a Node service, where essential tests run and Docker image builds for deployment in each triggered build. Migrating from our legacy CI tools, we are able to execute steps in parallel and utilize machines on demand to not only save usage costs, but increase processing needs when required.</p>\n<p>So, how can we \u201csupercharge\u201d this experience even\u00a0further?</p>\n<p>One of the first things we identified in Cloud Build was that it was not a complete CI tool. Rather, it was something that we could build our tools on top of. If we compare this to a CI service like Drone, Drone provides plugins which are added to the service, while Cloud Build allows for Docker images to run as individual steps in the build process. Cloud Build makes for easy development of helper steps, where Dockerfiles are already an adopted standard for deployment usages, as opposed to plugin development which are on a different platform. Another point to note was that anyone could contribute to the ecosystem of helper steps by uploading and pooling the maintenance of these steps in a single repository. It was also easier to use these helper steps, as opposed to plugin installation that requires service updates and access rights to the\u00a0VMs.</p>\n<p>Within a month of development, we\u2019ve came up with multitude of helper images that are common requirements across different service\u00a0builds.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/969/1*HmXtEZ6WR4tYPvvFLjkkDw.png\"><figcaption>Fig 5. A common repository for all helper\u00a0images</figcaption></figure><p>We\u2019ve discussed caching and retrieving before. But one secret to this was the usage of GCP Cloud Storage\u2019s Storage Bucket to store the cached assets. By providing the documentation to use these images, colleagues could quickly adopt the caching tools without any knowledge of the underlying bucket access management and bucket allocation across repositories.</p>\n<p>Another \u201csupercharged\u201d experience was making it easy to access private assets that we own. By using GCP Key Management Service (KMS), \u201cvgit\u201d and \u201cvdocker\u201d were created as specialised \u201cgit\u201d and \u201cdocker\u201d binaries in images\u00a0, which could access our private repositories in Github (such as initializing private sub-modules), and pushing or pulling docker images in Dockerhub in supporting migration of CI (without breaking deployment dependency through using GCP image repository alone). Usage pattern was similar to the public variant, making it easy to \u201cplug and play\u201d for each\u00a0build.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*xYupaUtWTDMeoshP_O2Alw.png\"><figcaption>Fig 6. Leveraging Cloud Functions to build various notifiers</figcaption></figure><p>Build notifications were also not available within the native Cloud Build. However, what was provided was the ability to subscribe to GCP Pub/Sub for on a \u201cCloud Build\u201d topic. From this, we built a Cloud Function based on NodeJS to publish to a list of class objects as seen in Fig 6. Currently, we support a SlackNotifier class, which is initialized with a slack channel \u201cwebhook\u201d, \u201crepoName\u201d regex to execute the logic on, and a \u201cmoduleName\u201d which is a separate list of modules that can be contributed by any developer to decide the display format of the pass/fail slack\u00a0message.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/263/1*sRp2YCcx1ipWI9A7vudSag.png\"><figcaption>Fig 7a. Cloud Build\u00a0badges</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/697/1*lunriC2fyT85PUsurPqjqQ.png\"><figcaption>Fig 7b. Cloud Build Github status as viewed in pull\u00a0requests</figcaption></figure><p>Other innovations here include status notifications and build badges, observed in Fig 7a/b above. The status notification gives an account of elapsed time for each step and indicates which step cause the failure if it occurs, allowing for a quick understanding of build issues. Build badges gives an account of the repository status for recent\u00a0builds.</p>\n<p>The notifications do not end here\u200a\u2014\u200aother innovations such as email alerts, or perhaps even paging alerts can be built and reused across different teams easily, with the current notification boilerplate that we have created\u00a0here.</p>\n<p>Lastly, another colleague of mine, Donovan, came up with the idea of parsing Cloud Build configuration files to create graphs for visual understanding the parallel execution of the steps. This was especially important to understanding projects with extensive and complex build steps. Building on his idea, a <a href=\"https://github.com/Weiyuan-Lane/cloudbuild_viz\">gem</a> was created to make the graph generation process easier to\u00a0perform:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/641/1*MeTP1bh03FXhr8hm2EWCOQ.png\"><figcaption>Fig 8. Graph visualization generated from sample Cloud Build configuration</figcaption></figure><p>All in all, we migrated from various CI tools to Cloud Build and solved our initial problems of latency and failures by increasing machine availability through each build running on a separate VM as managed CI. Through parallelizing build steps and controlling VM types, we optimize build time as compared to previous CI tools. Then, by using community assets and in-house development of helper content, we optimize both build time, feedback and development experience of builds even\u00a0further.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f97c8c09c63f\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/how-to-supercharge-continuous-integration-ci-tools-for-your-workplace-with-managed-ci-f97c8c09c63f\">How to supercharge Continuous Integration (CI) tools for your workplace with managed CI!</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>How we supercharge Continuous Integration (CI) tools at Rakuten\u00a0Viki!</h3>\n<p>Some time ago, my colleague and manager of our platform team at <a href=\"http://www.viki.com/\">Rakuten Viki</a>, Omkiran, started a company-wide conversation on re-hauling most of our entire infrastructure. This was done at a time where several of our services were deployed within Virtual Machines (VMs), essentially using Infrastructure as a Service (IaaS) services. The premise of the conversation was as follows\u00a0\u2014</p>\n<blockquote>If limited resources of an organisation, such as engineers, were to be tied down with solving low level tasks like managing the health and operability of various processes, how can we, as a company, spare resources as we scale up in services? How can we be able to automate our entire infrastructure, so that we can move on and innovate on newer technologies, such as Machine Learning?</blockquote>\n<p>The proposed solution was to make use of cloud providers, or more specifically to today\u2019s implementation, <strong>Google Cloud Platform\u00a0(GCP)</strong>.</p>\n<p>First, let us discuss why cloud providers are essential to meeting the highlighted premise\u00a0above:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/816/0*pG5zK5AZGLTeVkHw\"><figcaption>Fig 1. Comparing cloud service management requirements with physical solutions (<a href=\"https://jelastic.com/blog/growing-paas-market-benefits-for-enterprises-service-providers-and-developers-dailyhostnews/\">original article</a>)</figcaption></figure><ul>\n<li>Localized server racks suffer from the requirement to provide continuous support for physical assets. Hardware and software changes are challenges faced at this\u00a0level.</li>\n<li>VMs provided by IaaS providers assuage the hardware requirements in Fig 1. However, we\u2019ll still face software challenges like managing OS upgrades, updates and scaling to\u00a0demand.</li>\n</ul>\n<p>In the above, we see that both hardware and software pose challenges, to continue operations and ever-expanding business requirements of a growing organization. These challenges, at large, have already been solved by the industry as seen in Fig\u00a01.</p>\n<p><strong>This leads us to turn to cloud providers, which provide managed services, such as Platform as a Service (PaaS, so that we can free engineering resources and focus on solving challenges that are specific to growing our organization.</strong></p>\n<p><strong>But how does this change tie in with CI\u00a0tooling?</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/691/1*3_0oyZzlzLOrlZZKLGsfoA.png\"><figcaption>Fig 2. Why? Why?\u00a0Why?</figcaption></figure><p><strong>Firstly, what does CI mean to Viki:</strong><br>Services in Viki uses CI in the development part of our pipeline, often for automating the execution of new and regression tests to validate changes under multi-branch development. There are other usages pertaining to specific teams, such as pre-compiling assets for front-end services, and creating APK files for Android applications. Sometimes it goes beyond CI, using a limited form of Continuous Deployment (CD) for staging and canary environments in the delivery mindset of the\u00a0service.</p>\n<p><strong>How was CI done in Viki?</strong><br>Before, we used three different CI tools\u200a\u2014\u200atwo different versions of Drone, and Jenkins, in carrying out CI within the company. The CI tools were deployed as internal services, on top of provisioned VMs, similar to external services.</p>\n<p><strong>Why should CI change to be a managed tool, if it is an internal service and not tied to client (user) utilization?<br></strong>Client linked services have an explicit requirement to auto-scale as the users increase or decrease, which is already provided for in most PaaS services. In the case of CI, it is the \u201cservice\u201d for our services as its \u201cusers\u201d. As the number of services developed by the organization increases, the same issues faced by client services surface\u200a\u2014\u200anamely increased latency along with more frequent failures. We can vertically or horizontally scale the machines which power our CI to solve this issue, but reserving better or more instances does not make for cost efficiency where the engineering core is based in similar time zones (usage is low at night) and CI jobs differ in required processing power resulting in unpredictable fluctuations in usage. Horizontal auto-scaling is the solution here, but it would have to either be already supported by the CI, or we have to build custom logic to spin up and spin down VMs whenever it is required.</p>\n<p>Another issue was supporting multiple versions of CI or different CI tools altogether. Unfortunately, this was a result of incomplete migrations and incompatible instructions across versions - which leads to another familiar conundrum we have seen in Fig 1\u200a\u2014\u200amaintenance of different \u201cOS\u201d.</p>\n<p><strong>How does GCP solve these problems?</strong></p>\n<p>We looked at GCP\u2019s offering, <a href=\"https://cloud.google.com/cloud-build/\">Cloud Build</a>, for the problems we faced with present iterations of our\u00a0CI.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/proxy/1*p6Nc3zVTadBSQDd7cXvzDw.png\"><figcaption>Fig 3. Cloud\u00a0Build!</figcaption></figure><p>Firstly, I have to declare that Cloud Build by itself is not a complete CI tool. Instead, I would categorize it as the base which anyone can build kick-ass CI tools\u00a0on.</p>\n<p>Cloud Build works by providing a trigger configuration and build configuration. The trigger configuration is as\u00a0follows:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/615/1*s0EuORFo8X0rkndrsN992g.png\"><figcaption>Fig 4. Cloud Build trigger configuration</figcaption></figure><p>By linking to either Github, Bitbucket, or Google Code Repository, builds can be configured to trigger on watched files, or everything-but-ignored files, on top of selected branches (with regex to provide selection criteria, but mind you, no negative look-ahead supported at this point). Another option is providing a Dockerfile or a configuration file (YAML) specifically for Cloud Build, as the build configuration required to pass or fail a triggered build.</p>\n<p>For Viki, we opted to use the Cloud Build\u2019s configuration file as a common standard, as opposed to a Dockerfile, due to the fact that the \u201cbase images\u201d of the Cloud Build\u2019s configuration are essentially machine types (of differing processing power and number of cores). This solves our problem of latency and failures due to scaling issues, and even allow us to speed up individual builds for critical services as and when we\u00a0need.</p>\n<p>The following configuration shows a Node application with test and linter checks, along with building an image for deployment:</p>\n<pre>steps:<br>- id: retrieveNodeModules<br>  waitFor: ['-']<br>  name: gcr.io/${PROJECT_ID}/retrieve:1.0.0<br>  args: [$REPO_NAME, 'nodeModules']<br><br>- id: installTestNpm<br>  waitFor: ['retrieveNodeModules']<br>  name: 'gcr.io/cloud-builders/npm'<br>  args: ['--loglevel=error', 'install', '--prefer-offline']</pre>\n<pre>- id: runJasmineTests<br>  waitFor: ['installTestNpm']<br>  name: 'gcr.io/cloud-builders/npm'<br>  args: ['run', 'jasmine']<br><br>- id: runLinterChecks<br>  waitFor: ['installTestNpm']<br>  name: 'gcr.io/cloud-builders/npm'<br>  args: ['run', 'linter']<br><br>- id: cacheNodeModules<br>  waitFor: ['installTestNpm']<br>  name: gcr.io/${PROJECT_ID}/cache:1.0.0<br>  args: [$REPO_NAME, 'nodeModules']</pre>\n<pre>- id: dockerBuild<br>  waitFor: ['runJasmineTests', 'runLinterChecks']<br>  name: 'gcr.io/cloud-builders/docker'<br>  args: ['build', '-f', 'Dockerfile', '-t', 'gcr.io/$PROJECT_ID/project:$SHORT_SHA', '.']<br><br>- id: dockerPush<br>  waitFor: ['dockerBuild']<br>  name: 'gcr.io/cloud-builders/docker'<br>  args: ['push', 'gcr.io/$PROJECT_ID/project:$SHORT_SHA', '.']</pre>\n<pre>timeout: 1800s<br><br>options:<br>  machineType: 'N1_HIGHCPU_8'</pre>\n<p>From the above example, we see a few configuration properties. The first thing we\u2019ll touch on would be the steps\u2019 properties for the build. \u201cname\u201d and \u201cargs\u201d are synonymous to the common usage of Docker images, in identifying the image used and arguments in spinning up the image as a dockerized container. \u201cid\u201d and \u201cwaitFor\u201d creates the build dependency between steps, allowing for steps to execute in parallel as discussed before.</p>\n<p>Another property here is \u201cmachineType\u201d, where one can select the machine to run the build on. This stacks with the ability to run builds in parallel, whereby selecting a multi-core machine will help to speed up build\u00a0time.</p>\n<p>For some CI tools, a single build run steps serially in a unique environment. This is a result of each service with their own requirements. By using Cloud Build\u2019s steps as seen above, this translates to the advantage that we can create helper steps which can be shared across different services\u200a\u2014\u200asuch as caching utility to speed up build times, or deployment steps to Kubernetes using helm to staging or canary clusters.</p>\n<p>Usage of Cloud Build expands even further with the use of GCP. In the above configuration YAML file, the Docker images of the different steps come from both public and private image repositories hosted on GCP. Through the use of service accounts, we can enable Cloud Build to access private images, without having to configure and manage complex access management, such as creating and uploading public-private SSH key-pairs.</p>\n<p>Better still, only the administrators could perform access management rights\u2019 assignments, and regular developers possess only read privileges. This safeguards our CI from failures caused by accidental changes.</p>\n<p>From the above, we have created an automated build pipeline for a Node service, where essential tests run and Docker image builds for deployment in each triggered build. Migrating from our legacy CI tools, we are able to execute steps in parallel and utilize machines on demand to not only save usage costs, but increase processing needs when required.</p>\n<p>So, how can we \u201csupercharge\u201d this experience even\u00a0further?</p>\n<p>One of the first things we identified in Cloud Build was that it was not a complete CI tool. Rather, it was something that we could build our tools on top of. If we compare this to a CI service like Drone, Drone provides plugins which are added to the service, while Cloud Build allows for Docker images to run as individual steps in the build process. Cloud Build makes for easy development of helper steps, where Dockerfiles are already an adopted standard for deployment usages, as opposed to plugin development which are on a different platform. Another point to note was that anyone could contribute to the ecosystem of helper steps by uploading and pooling the maintenance of these steps in a single repository. It was also easier to use these helper steps, as opposed to plugin installation that requires service updates and access rights to the\u00a0VMs.</p>\n<p>Within a month of development, we\u2019ve came up with multitude of helper images that are common requirements across different service\u00a0builds.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/969/1*HmXtEZ6WR4tYPvvFLjkkDw.png\"><figcaption>Fig 5. A common repository for all helper\u00a0images</figcaption></figure><p>We\u2019ve discussed caching and retrieving before. But one secret to this was the usage of GCP Cloud Storage\u2019s Storage Bucket to store the cached assets. By providing the documentation to use these images, colleagues could quickly adopt the caching tools without any knowledge of the underlying bucket access management and bucket allocation across repositories.</p>\n<p>Another \u201csupercharged\u201d experience was making it easy to access private assets that we own. By using GCP Key Management Service (KMS), \u201cvgit\u201d and \u201cvdocker\u201d were created as specialised \u201cgit\u201d and \u201cdocker\u201d binaries in images\u00a0, which could access our private repositories in Github (such as initializing private sub-modules), and pushing or pulling docker images in Dockerhub in supporting migration of CI (without breaking deployment dependency through using GCP image repository alone). Usage pattern was similar to the public variant, making it easy to \u201cplug and play\u201d for each\u00a0build.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*xYupaUtWTDMeoshP_O2Alw.png\"><figcaption>Fig 6. Leveraging Cloud Functions to build various notifiers</figcaption></figure><p>Build notifications were also not available within the native Cloud Build. However, what was provided was the ability to subscribe to GCP Pub/Sub for on a \u201cCloud Build\u201d topic. From this, we built a Cloud Function based on NodeJS to publish to a list of class objects as seen in Fig 6. Currently, we support a SlackNotifier class, which is initialized with a slack channel \u201cwebhook\u201d, \u201crepoName\u201d regex to execute the logic on, and a \u201cmoduleName\u201d which is a separate list of modules that can be contributed by any developer to decide the display format of the pass/fail slack\u00a0message.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/263/1*sRp2YCcx1ipWI9A7vudSag.png\"><figcaption>Fig 7a. Cloud Build\u00a0badges</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/697/1*lunriC2fyT85PUsurPqjqQ.png\"><figcaption>Fig 7b. Cloud Build Github status as viewed in pull\u00a0requests</figcaption></figure><p>Other innovations here include status notifications and build badges, observed in Fig 7a/b above. The status notification gives an account of elapsed time for each step and indicates which step cause the failure if it occurs, allowing for a quick understanding of build issues. Build badges gives an account of the repository status for recent\u00a0builds.</p>\n<p>The notifications do not end here\u200a\u2014\u200aother innovations such as email alerts, or perhaps even paging alerts can be built and reused across different teams easily, with the current notification boilerplate that we have created\u00a0here.</p>\n<p>Lastly, another colleague of mine, Donovan, came up with the idea of parsing Cloud Build configuration files to create graphs for visual understanding the parallel execution of the steps. This was especially important to understanding projects with extensive and complex build steps. Building on his idea, a <a href=\"https://github.com/Weiyuan-Lane/cloudbuild_viz\">gem</a> was created to make the graph generation process easier to\u00a0perform:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/641/1*MeTP1bh03FXhr8hm2EWCOQ.png\"><figcaption>Fig 8. Graph visualization generated from sample Cloud Build configuration</figcaption></figure><p>All in all, we migrated from various CI tools to Cloud Build and solved our initial problems of latency and failures by increasing machine availability through each build running on a separate VM as managed CI. Through parallelizing build steps and controlling VM types, we optimize build time as compared to previous CI tools. Then, by using community assets and in-house development of helper content, we optimize both build time, feedback and development experience of builds even\u00a0further.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f97c8c09c63f\" width=\"1\" height=\"1\"><hr>\n<p><a href=\"https://blog.viki.com/how-to-supercharge-continuous-integration-ci-tools-for-your-workplace-with-managed-ci-f97c8c09c63f\">How to supercharge Continuous Integration (CI) tools for your workplace with managed CI!</a> was originally published in <a href=\"https://blog.viki.com/\">Viki Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["cloud-build","engineering","continuous-integration","google-cloud-platform","docker"]}]}